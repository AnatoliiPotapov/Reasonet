{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Части, специфичные для Reasonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:All works\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.info('All works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from allennlp.common import Params\n",
    "#from allennlp.modules.similarity_functions.similarity_function import SimilarityFunction\n",
    "from allennlp.modules.similarity_function import SimilarityFunction\n",
    "from allennlp.nn import Activation\n",
    "from torch.autograd import Variable\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.nn.util import weighted_sum\n",
    "import allennlp.nn.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CUDA_wrapper(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchwise_index(array_batch, index_batch):\n",
    "    assert index_batch.dim() == 1\n",
    "    assert array_batch.size(0) == index_batch.size(0)\n",
    "    index_batch_one_hot = CUDA_wrapper(\n",
    "        autograd.Variable(torch.ByteTensor(array_batch.size()).zero_(), requires_grad=False)\n",
    "    )\n",
    "    index_batch_one_hot.scatter_(1, index_batch.data.unsqueeze(-1), 1)\n",
    "    return array_batch[index_batch_one_hot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention over memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionCoefProvider(object):\n",
    "    \"\"\"\n",
    "    Attention coef provider\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 similarity_function: SimilarityFunction,\n",
    "                 normalize: bool) -> None:\n",
    "        self._similarity_function = similarity_function\n",
    "        self._attention = Attention(similarity_function, normalize=normalize)\n",
    "        super(AttentionCoefProvider, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        return self._attention(state, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionOverMemory(object):\n",
    "    \"\"\"\n",
    "    Attention over memory\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 coef_provider: AttentionCoefProvider) -> None:\n",
    "        self._coef_provider = coef_provider\n",
    "        super(AttentionOverMemory, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        attn_coefficients = self._coef_provider.forward(state, memory)\n",
    "        return weighted_sum(memory, attn_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AttentionProvider = AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateController(object):\n",
    "\n",
    "    def __init__(self, module: torch.nn.modules.RNNCell) -> None:\n",
    "        super(StateController, self).__init__()\n",
    "        self._module = module\n",
    "        self.hidden_state = None\n",
    "        try:\n",
    "            if not self._module.batch_first:\n",
    "                raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._module.input_size\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._module.hidden_size\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.Tensor,\n",
    "                hidden_state: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        if hidden_state is None:\n",
    "            if not self.hidden_state is None:\n",
    "                hidden_state = self.hidden_state\n",
    "            else:\n",
    "                raise ConfigurationError(\"Hidden state must be specified!\")\n",
    "                \n",
    "        self.hidden_state = self._module(inputs, hidden_state)\n",
    "        return self.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_controller = StateController(torch.nn.GRUCell(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TerminationGate(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim: int) -> None:\n",
    "        super(TerminationGate, self).__init__()\n",
    "        self._hiden_dim = hidden_dim\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, hidden_state: torch.Tensor) -> None:\n",
    "        tensor_output = util.last_dim_softmax(self.linear(hidden_state))\n",
    "        last_dim = tensor_output.dim() - 1\n",
    "        return torch.chunk(tensor_output, 2, last_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasonet inner controller logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReasoningProcess(object):\n",
    "    \n",
    "    def __init__(self, timesteps:int,\n",
    "                 attention_provider: AttentionProvider,\n",
    "                 state_controller: StateController) -> None:\n",
    "        \n",
    "        self._timesteps = timesteps\n",
    "        self._attention_provider = attention_provider\n",
    "        self._state_controller = state_controller\n",
    "        \n",
    "    def forward(self, initial_hidden_state: torch.Tensor,\n",
    "                memory: torch.Tensor):\n",
    "        \n",
    "        # use initial_hidden state to perform first state of computations\n",
    "        attn = self._attention_provider.forward(initial_hidden_state, memory)\n",
    "        hidden_state = self._state_controller.forward(attn, initial_hidden_state)\n",
    "        \n",
    "        if self._timesteps > 1:\n",
    "            for i in range(self._timesteps-1):\n",
    "                attn = self._attention_provider.forward(hidden_state, memory)\n",
    "                hidden_state = self._state_controller.forward(attn)\n",
    "                \n",
    "        return hidden_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReasoningProcessStep(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 attention_provider: AttentionProvider,\n",
    "                 state_controller: StateController) -> None:\n",
    "        \n",
    "        self._attention_provider = attention_provider\n",
    "        self._state_controller = state_controller\n",
    "        \n",
    "    def forward(self, initial_hidden_state: torch.Tensor,\n",
    "                memory: torch.Tensor):\n",
    "        \n",
    "        # use initial_hidden state to perform first state of computations\n",
    "        attn = self._attention_provider.forward(initial_hidden_state, memory)\n",
    "        hidden_state = self._state_controller.forward(attn, initial_hidden_state)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reasoner = ReasoningProcess(5, \n",
    "                            AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True)),\n",
    "                            StateController(torch.nn.GRUCell(300, 300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2SeqWrapper, that returns (all_states, last_state) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n",
    "from allennlp.nn.util import sort_batch_by_length, get_lengths_from_binary_sequence_mask\n",
    "\n",
    "\n",
    "class PytorchLastStateSeq2SeqWrapper(Seq2SeqEncoder):\n",
    "    \"\"\"\n",
    "    Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at\n",
    "    the last time step for every layer.  We just want the first one as a single output.  This\n",
    "    wrapper pulls out that output, and adds a :func:`get_output_dim` method, which is useful if you\n",
    "    want to, e.g., define a linear + softmax layer on top of this to get some distribution over a\n",
    "    set of labels.  The linear layer needs to know its input dimension before it is called, and you\n",
    "    can get that from ``get_output_dim``.\n",
    "\n",
    "    In order to be wrapped with this wrapper, a class must have the following members:\n",
    "\n",
    "        - ``self.input_size: int``\n",
    "        - ``self.hidden_size: int``\n",
    "        - ``def forward(inputs: PackedSequence, hidden_state: torch.autograd.Variable) ->\n",
    "          Tuple[PackedSequence, torch.autograd.Variable]``.\n",
    "        - ``self.bidirectional: bool`` (optional)\n",
    "\n",
    "    This is what pytorch's RNN's look like - just make sure your class looks like those, and it\n",
    "    should work.\n",
    "\n",
    "    Note that we *require* you to pass sequence lengths when you call this module, to avoid subtle\n",
    "    bugs around masking.  If you already have a ``PackedSequence`` you can pass ``None`` as the\n",
    "    second parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, module: torch.nn.modules.RNNBase) -> None:\n",
    "        super(PytorchLastStateSeq2SeqWrapper, self).__init__()\n",
    "        self._module = module\n",
    "        try:\n",
    "            if not self._module.batch_first:\n",
    "                raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._module.input_size\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        try:\n",
    "            is_bidirectional = self._module.bidirectional\n",
    "        except AttributeError:\n",
    "            is_bidirectional = False\n",
    "        return self._module.hidden_size * (2 if is_bidirectional else 1)\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                hidden_state: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        if mask is None:\n",
    "            return self._module(inputs, hidden_state)[0]\n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices = sort_batch_by_length(inputs,\n",
    "                                                                                           sequence_lengths)\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs,\n",
    "                                                     sorted_sequence_lengths.data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "\n",
    "        # Actually call the module on the sorted PackedSequence.\n",
    "        packed_sequence_output, state = self._module(packed_sequence_input, hidden_state)\n",
    "\n",
    "        # Deal with the fact the LSTM state is a tuple of (state, memory).\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        # Restore the original indices and return the final state of the\n",
    "        # top layer. Pytorch's recurrent layers return state in the form\n",
    "        # (num_layers * num_directions, batch_size, hidden_size) regardless\n",
    "        # of the 'batch_first' flag, so we transpose, extract the relevant\n",
    "        # layer state (both forward and backward if using bidirectional layers)\n",
    "        # and return them as a single (batch_size, self.get_output_dim()) tensor.\n",
    "\n",
    "        # now of shape: (batch_size, num_layers * num_directions, hidden_size).\n",
    "        unsorted_state = state.transpose(0, 1).index_select(0, restoration_indices)\n",
    "\n",
    "        # Extract the last hidden vector, including both forward and backward states\n",
    "        # if the cell is bidirectional. Then reshape by concatenation (in the case\n",
    "        # we have bidirectional states) or just squash the 1st dimension in the non-\n",
    "        # bidirectional case. Return tensor has shape (batch_size, hidden_size * num_directions).\n",
    "        try:\n",
    "            last_state_index = 2 if self._module.bidirectional else 1\n",
    "        except AttributeError:\n",
    "            last_state_index = 1\n",
    "        last_layer_state = unsorted_state[:, -last_state_index:, :]\n",
    "\n",
    "        unpacked_sequence_tensor, _ = pad_packed_sequence(packed_sequence_output, batch_first=True)\n",
    "        # Restore the original indices and return the sequence.\n",
    "        return unpacked_sequence_tensor.index_select(0, restoration_indices), last_layer_state.contiguous().view([-1, self.get_output_dim()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.modules.augmented_lstm import AugmentedLstm\n",
    "#from allennlp.modules.seq2seq_encoders.intra_sentence_attention import IntraSentenceAttentionEncoder\n",
    "from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import PytorchSeq2SeqWrapper\n",
    "#from allennlp.modules.seq2seq_encoders.pytorch_last_state_seq2seq_wrapper import PytorchLastStateSeq2SeqWrapper\n",
    "from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n",
    "from allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\n",
    "\n",
    "class Seq2SeqWrapperWithLastState:\n",
    "    \"\"\"\n",
    "    For :class:`Registrable` we need to have a ``Type[Seq2SeqEncoder]`` as the value registered for each\n",
    "    key.  What that means is that we need to be able to ``__call__`` these values (as is done with\n",
    "    ``__init__`` on the class), and be able to call ``from_params()`` on the value.\n",
    "\n",
    "    In order to accomplish this, we have two options: (1) we create a ``Seq2SeqEncoder`` class for\n",
    "    all of pytorch's RNN modules individually, with our own parallel classes that we register in\n",
    "    the registry; or (2) we wrap pytorch's RNNs with something that `mimics` the required\n",
    "    API.  We've gone with the second option here.\n",
    "\n",
    "    This is a two-step approach: first, we have the :class:`PytorchSeq2SeqWrapper` class that handles\n",
    "    the interface between a pytorch RNN and our ``Seq2SeqEncoder`` API.  Our ``PytorchSeq2SeqWrapper``\n",
    "    takes an instantiated pytorch RNN and just does some interface changes.  Second, we need a way\n",
    "    to create one of these ``PytorchSeq2SeqWrappers``, with an instantiated pytorch RNN, from the\n",
    "    registry.  That's what this ``_Wrapper`` does.  The only thing this class does is instantiate\n",
    "    the pytorch RNN in a way that's compatible with ``Registrable``, then pass it off to the\n",
    "    ``PytorchSeq2SeqWrapper`` class.\n",
    "\n",
    "    When you instantiate a ``_Wrapper`` object, you give it an ``RNNBase`` subclass, which we save\n",
    "    to ``self``.  Then when called (as if we were instantiating an actual encoder with\n",
    "    ``Encoder(**params)``, or with ``Encoder.from_params(params)``), we pass those parameters\n",
    "    through to the ``RNNBase`` constructor, then pass the instantiated pytorch RNN to the\n",
    "    ``PytorchSeq2SeqWrapper``.  This lets us use this class in the registry and have everything just\n",
    "    work.\n",
    "    \"\"\"\n",
    "    PYTORCH_MODELS = [torch.nn.GRU, torch.nn.LSTM, torch.nn.RNN]\n",
    "\n",
    "    def __init__(self, module_class: Type[torch.nn.modules.RNNBase], return_last_state=False) -> None:\n",
    "        self._return_last_state = return_last_state\n",
    "        self._module_class = module_class\n",
    "\n",
    "    def __call__(self, **kwargs) -> PytorchSeq2SeqWrapper:\n",
    "        return self.from_params(Params(kwargs))\n",
    "\n",
    "    def from_params(self, params: Params) -> PytorchSeq2SeqWrapper:\n",
    "        if not params.pop('batch_first', True):\n",
    "            raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        if self._module_class in self.PYTORCH_MODELS:\n",
    "            params['batch_first'] = True\n",
    "        module = self._module_class(**params.as_dict())\n",
    "        if not self._return_last_state:\n",
    "            return PytorchSeq2SeqWrapper(module)\n",
    "        else:\n",
    "            return PytorchLastStateSeq2SeqWrapper(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Seq2SeqWrapperWithLastState at 0x7f62a9a29588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq2SeqEncoder.register(\"l_lstm\")(Seq2SeqWrapperWithLastState(torch.nn.LSTM, return_last_state=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "import pyhocon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"squad\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"character_tokenizer\": {\n",
    "          \"byte_encoding\": \"utf-8\",\n",
    "          \"start_tokens\": [259],\n",
    "          \"end_tokens\": [260]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"./squad/train-v1.1.json\",\n",
    "  \"validation_data_path\": \"./squad/dev-v1.1.json\",\n",
    "  \"model\": {\n",
    "    \"type\": \"reasonet_dev\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"embedding\",\n",
    "        \"pretrained_file\": \"./glove/glove.6B.100d.txt.gz\",\n",
    "        \"embedding_dim\": 100,\n",
    "        \"trainable\": false\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"character_encoding\",\n",
    "        \"embedding\": {\n",
    "          \"num_embeddings\": 262,\n",
    "          \"embedding_dim\": 16\n",
    "        },\n",
    "        \"encoder\": {\n",
    "          \"type\": \"cnn\",\n",
    "          \"embedding_dim\": 16,\n",
    "          \"num_filters\": 100,\n",
    "          \"ngram_filter_sizes\": [5]\n",
    "        },\n",
    "        \"dropout\": 0.2\n",
    "      }\n",
    "    },\n",
    "    \"num_highway_layers\": 2,\n",
    "    \"state_controller\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"phrase_layer\": {\n",
    "      \"type\": \"l_lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"similarity_function\": {\n",
    "      \"type\": \"linear\",\n",
    "      \"combination\": \"x,y,x*y\",\n",
    "      \"tensor_1_dim\": 200,\n",
    "      \"tensor_2_dim\": 200\n",
    "    },\n",
    "    \"modeling_layer\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 800,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"dropout\": 0.2\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"sorting_keys\": [[\"passage\", \"num_tokens\"], [\"question\", \"num_tokens\"]],\n",
    "    \"batch_size\": 40\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 20,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 10,\n",
    "    \"validation_metric\": \"+em\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"learning_rate_scheduler\":  {\n",
    "      \"type\": \"reduce_on_plateau\",\n",
    "      \"factor\": 0.5,\n",
    "      \"mode\": \"max\",\n",
    "      \"patience\": 2,\n",
    "\n",
    "    },\n",
    "    \"no_tqdm\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"betas\": [0.9, 0.9]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = Params(pyhocon.ConfigFactory.parse_string(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.common.params.Params at 0x7f62a99e1240>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import Highway, MatrixAttention\n",
    "from allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\n",
    "from allennlp.nn import util\n",
    "from allennlp.nn.initializers import InitializerApplicator\n",
    "from allennlp.training.regularizers import RegularizerApplicator\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "# Should import this, but can't:\n",
    "#from allennlp.training.metrics import SquadEmAndF1\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.nn.util import weighted_sum\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common import squad_eval\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "\n",
    "\n",
    "@Metric.register(\"squad\")\n",
    "class SquadEmAndF1(Metric):\n",
    "    \"\"\"\n",
    "    This :class:`Metric` takes the best span string computed by a model, along with the answer\n",
    "    strings labeled in the data, and computed exact match and F1 score using the official SQuAD\n",
    "    evaluation script.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._total_em = 0.0\n",
    "        self._total_f1 = 0.0\n",
    "        self._count = 0\n",
    "\n",
    "    @overrides\n",
    "    def __call__(self, best_span_string, answer_strings):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : ``float``\n",
    "            The value to average.\n",
    "        \"\"\"\n",
    "        exact_match = squad_eval.metric_max_over_ground_truths(\n",
    "                squad_eval.exact_match_score,\n",
    "                best_span_string,\n",
    "                answer_strings)\n",
    "        f1_score = squad_eval.metric_max_over_ground_truths(\n",
    "                squad_eval.f1_score,\n",
    "                best_span_string,\n",
    "                answer_strings)\n",
    "        self._total_em += exact_match\n",
    "        self._total_f1 += f1_score\n",
    "        self._count += 1\n",
    "\n",
    "    @overrides\n",
    "    def get_metric(self, reset: bool = False) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Average exact match and F1 score (in that order) as computed by the official SQuAD script\n",
    "        over all inputs.\n",
    "        \"\"\"\n",
    "        exact_match = self._total_em / self._count if self._count > 0 else 0\n",
    "        f1_score = self._total_f1 / self._count if self._count > 0 else 0\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return exact_match, f1_score\n",
    "\n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self._total_em = 0.0\n",
    "        self._total_f1 = 0.0\n",
    "        self._count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"reasonet_dev\")\n",
    "class Reasonet(Model):\n",
    "    \"\"\"\n",
    "    This class implements Minjoon Seo's `Bidirectional Attention Flow model\n",
    "    <https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/7586b7cca1deba124af80609327395e613a20e9d>`_\n",
    "    for answering reading comprehension questions (ICLR 2017).\n",
    "\n",
    "    The basic layout is pretty simple: encode words as a combination of word embeddings and a\n",
    "    character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of\n",
    "    attentions to put question information into the passage word representations (this is the only\n",
    "    part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and\n",
    "    do a softmax over span start and span end.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``\n",
    "    text_field_embedder : ``TextFieldEmbedder``\n",
    "        Used to embed the ``question`` and ``passage`` ``TextFields`` we get as input to the model.\n",
    "    num_highway_layers : ``int``\n",
    "        The number of highway layers to use in between embedding the input and passing it through\n",
    "        the phrase layer.\n",
    "    phrase_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between embedding tokens\n",
    "        and doing the bidirectional attention.\n",
    "    attention_similarity_function : ``SimilarityFunction``\n",
    "        The similarity function that we will use when comparing encoded passage and question\n",
    "        representations.\n",
    "    modeling_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between the bidirectional\n",
    "        attention and predicting span start and end.\n",
    "    span_end_encoder : ``Seq2SeqEncoder``\n",
    "        The encoder that we will use to incorporate span start predictions into the passage state\n",
    "        before predicting span end.\n",
    "    dropout : ``float``, optional (default=0.2)\n",
    "        If greater than 0, we will apply dropout with this probability after all encoders (pytorch\n",
    "        LSTMs do not apply dropout to their last layer).\n",
    "    mask_lstms : ``bool``, optional (default=True)\n",
    "        If ``False``, we will skip passing the mask to the LSTM layers.  This gives a ~2x speedup,\n",
    "        with only a slight performance decrease, if any.  We haven't experimented much with this\n",
    "        yet, but have confirmed that we still get very similar performance with much faster\n",
    "        training times.  We still use the mask for all softmaxes, but avoid the shuffling that's\n",
    "        required when using masking with pytorch LSTMs.\n",
    "    evaluation_json_file : ``str``, optional\n",
    "        If given, we will load this JSON into memory and use it to compute official metrics\n",
    "        against.  We need this separately from the validation dataset, because the official metrics\n",
    "        use all of the annotations, while our dataset reader picks the most frequent one.\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 num_highway_layers: int,\n",
    "                 state_controller: Seq2SeqEncoder,\n",
    "                 phrase_layer: Seq2SeqEncoder,\n",
    "                 attention_similarity_function: SimilarityFunction,\n",
    "                 modeling_layer: Seq2SeqEncoder,\n",
    "                 dropout: float = 0.2,\n",
    "                 mask_lstms: bool = True,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None,\n",
    "                 max_timesteps: int = 5) -> None:\n",
    "        #super(Reasonet, self).__init__(vocab, regularizer)\n",
    "        super(Reasonet, self).__init__(vocab)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._highway_layer = TimeDistributed(Highway(text_field_embedder.get_output_dim(),\n",
    "                                                      num_highway_layers))\n",
    "\n",
    "        self._state_controller = state_controller\n",
    "        self._phrase_layer = phrase_layer\n",
    "        self._matrix_attention = MatrixAttention(attention_similarity_function)\n",
    "        self._modeling_layer = modeling_layer\n",
    "\n",
    "        encoding_dim = phrase_layer.get_output_dim()\n",
    "        modeling_dim = modeling_layer.get_output_dim()\n",
    "        state_controller_dim = modeling_dim\n",
    "\n",
    "        similarity_function = CUDA_wrapper(BilinearSimilarity(state_controller_dim, modeling_dim))\n",
    "        state_rnn = CUDA_wrapper(torch.nn.GRUCell(state_controller_dim, state_controller_dim))\n",
    "\n",
    "        self.termination_gate = CUDA_wrapper(TerminationGate(state_controller_dim))\n",
    "\n",
    "        coef_provider = AttentionCoefProvider(similarity_function, True)\n",
    "\n",
    "        self.reasoner_step = ReasoningProcessStep(\n",
    "            AttentionOverMemory(coef_provider),\n",
    "            StateController(state_rnn)\n",
    "        )\n",
    "        self.max_timesteps = max_timesteps\n",
    "        \n",
    "        span_start_input_dim = 2*modeling_dim\n",
    "        self._span_start_predictor = TimeDistributed(torch.nn.Linear(span_start_input_dim, 1))\n",
    "\n",
    "        span_end_input_dim = 2*modeling_dim\n",
    "        self._span_end_predictor = TimeDistributed(torch.nn.Linear(span_end_input_dim, 1))\n",
    "\n",
    "        self._span_start_accuracy = CategoricalAccuracy()\n",
    "        self._span_end_accuracy = CategoricalAccuracy()\n",
    "        self._span_accuracy = BooleanAccuracy()\n",
    "        self._squad_metrics = SquadEmAndF1()\n",
    "        if dropout > 0:\n",
    "            self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self._dropout = lambda x: x\n",
    "        self._mask_lstms = mask_lstms\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(self,  # type: ignore\n",
    "                question: Dict[str, torch.LongTensor],\n",
    "                passage: Dict[str, torch.LongTensor],\n",
    "                span_start: torch.IntTensor = None,\n",
    "                span_end: torch.IntTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.\n",
    "        passage : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.  The model assumes that this passage contains the answer to the\n",
    "            question, and predicts the beginning and ending positions of the answer within the\n",
    "            passage.\n",
    "        span_start : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            beginning position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        span_end : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            ending position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        metadata : ``List[Dict[str, Any]]``, optional\n",
    "            If present, this should contain the question ID, original passage text, and token\n",
    "            offsets into the passage for each instance in the batch.  We use this for computing\n",
    "            official metrics using the official SQuAD evaluation script.  The length of this list\n",
    "            should be the batch size, and each dictionary should have the keys ``id``,\n",
    "            ``original_passage``, and ``token_offsets``.  If you only want the best span string and\n",
    "            don't care about official metrics, you can omit the ``id`` key.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        span_start_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span start position.\n",
    "        span_start_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_start_logits)``.\n",
    "        span_end_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span end position (inclusive).\n",
    "        span_end_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_end_logits)``.\n",
    "        best_span : torch.IntTensor\n",
    "            The result of a constrained inference over ``span_start_logits`` and\n",
    "            ``span_end_logits`` to find the most probable span.  Shape is ``(batch_size, 2)``.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        best_span_str : List[str]\n",
    "            If sufficient metadata was provided for the instances in the batch, we also return the\n",
    "            string from the original passage that the model thinks is the best answer to the\n",
    "            question.\n",
    "        \"\"\"\n",
    "        embedded_question = self._highway_layer(self._text_field_embedder(question))\n",
    "        embedded_passage = self._highway_layer(self._text_field_embedder(passage))\n",
    "        batch_size = embedded_question.size(0)\n",
    "        passage_length = embedded_passage.size(1)\n",
    "        question_mask = util.get_text_field_mask(question).float()\n",
    "        passage_mask = util.get_text_field_mask(passage).float()\n",
    "        question_lstm_mask = question_mask if self._mask_lstms else None\n",
    "        passage_lstm_mask = passage_mask if self._mask_lstms else None\n",
    "\n",
    "        # We use question_last_state to initialize state controller\n",
    "        question_encoding, question_last_state = self._phrase_layer(embedded_question, question_lstm_mask)\n",
    "        passage_encoding, _ =  self._phrase_layer(embedded_passage, passage_lstm_mask)\n",
    "\n",
    "        encoded_question = self._dropout(question_encoding)\n",
    "        encoded_passage = self._dropout(passage_encoding)\n",
    "        encoding_dim = encoded_question.size(-1)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_attention = util.last_dim_softmax(passage_question_similarity, question_mask)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n",
    "\n",
    "        # We replace masked values with something really negative here, so they don't affect the\n",
    "        # max below.\n",
    "        masked_similarity = util.replace_masked_values(passage_question_similarity,\n",
    "                                                       question_mask.unsqueeze(1),\n",
    "                                                       -1e7)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_similarity = masked_similarity.max(dim=-1)[0].squeeze(-1)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_attention = util.masked_softmax(question_passage_similarity, passage_mask)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        question_passage_vector = util.weighted_sum(encoded_passage, question_passage_attention)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        tiled_question_passage_vector = question_passage_vector.unsqueeze(1).expand(batch_size,\n",
    "                                                                                    passage_length,\n",
    "                                                                                    encoding_dim)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, encoding_dim * 4)\n",
    "        final_merged_passage = torch.cat([encoded_passage,\n",
    "                                          passage_question_vectors,\n",
    "                                          encoded_passage * passage_question_vectors,\n",
    "                                          encoded_passage * tiled_question_passage_vector],\n",
    "                                         dim=-1)\n",
    "\n",
    "        modeled_passage = self._dropout(self._modeling_layer(final_merged_passage, passage_lstm_mask))\n",
    "        modeling_dim = modeled_passage.size(-1)\n",
    "\n",
    "        # !!! modelled passage = M\n",
    "        M = modeled_passage\n",
    "        \n",
    "        no_answer_yet = CUDA_wrapper(torch.ones(batch_size).byte())\n",
    "        \n",
    "        span_start_logits_final = CUDA_wrapper(torch.zeros(batch_size, passage_length))\n",
    "        span_end_logits_final = CUDA_wrapper(torch.zeros(batch_size, passage_length))\n",
    "        \n",
    "        expected_reward = 0\n",
    "        reasoner_last_state = question_last_state\n",
    "        proceed_until_now_prob = 1.\n",
    "        for step in range(self.max_timesteps):\n",
    "            reasoner_last_state = self.reasoner_step.forward(reasoner_last_state, M)\n",
    "            proceed_prob, stop_prob = self.termination_gate.forward(reasoner_last_state)\n",
    "            \n",
    "            # Shape: (batch_size, passage_length, modeling_dim)\n",
    "            tiled_reasoner_last_state = reasoner_last_state.unsqueeze(1).expand(\n",
    "                batch_size, passage_length, modeling_dim\n",
    "            )\n",
    "\n",
    "            # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))\n",
    "            answer_ready_representation = self._dropout(\n",
    "                torch.cat([modeled_passage, modeled_passage * tiled_reasoner_last_state], dim=-1)\n",
    "            )\n",
    "\n",
    "            # ! Start prediction\n",
    "\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_start_logits = self._span_start_predictor(answer_ready_representation).squeeze(-1)\n",
    "\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_end_logits = self._span_end_predictor(answer_ready_representation).squeeze(-1)\n",
    "\n",
    "            # ! End prediction\n",
    "\n",
    "            if step < self.max_timesteps - 1:\n",
    "                may_be_answer_now = torch.bernoulli(stop_prob.data.squeeze()).byte()\n",
    "            else:\n",
    "                may_be_answer_now = CUDA_wrapper(torch.ones(batch_size).byte())\n",
    "            answer_now = may_be_answer_now * no_answer_yet\n",
    "            no_answer_yet = no_answer_yet * (1 - answer_now)\n",
    "            \n",
    "            span_start_logits_final += answer_now.unsqueeze(-1).float() * span_start_logits.data\n",
    "            span_end_logits_final += answer_now.unsqueeze(-1).float() * span_end_logits.data\n",
    "\n",
    "            span_start_logits = util.replace_masked_values(span_start_logits, passage_mask, -1e7)\n",
    "            span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)\n",
    "            if span_start is not None:\n",
    "                #loss = nll_loss(util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))\n",
    "                #loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))\n",
    "                reward = batchwise_index(util.masked_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))\n",
    "                reward += batchwise_index(util.masked_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))\n",
    "                expected_reward += torch.mean(reward * stop_prob * proceed_until_now_prob)\n",
    "            proceed_until_now_prob = proceed_until_now_prob * proceed_prob\n",
    "\n",
    "        span_start_logits = Variable(span_start_logits_final, requires_grad=False)\n",
    "        span_end_logits = Variable(span_end_logits_final, requires_grad=False)\n",
    "        \n",
    "        span_start_logits = util.replace_masked_values(span_start_logits, passage_mask, -1e7)\n",
    "        span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)\n",
    "            \n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_start_probs = util.masked_softmax(span_start_logits, passage_mask)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_end_probs = util.masked_softmax(span_end_logits, passage_mask)\n",
    "\n",
    "        best_span = self._get_best_span(span_start_logits, span_end_logits)\n",
    "\n",
    "        output_dict = {\n",
    "            \"span_start_logits\": span_start_logits,\n",
    "            \"span_start_probs\": span_start_probs,\n",
    "            \"span_end_logits\": span_end_logits,\n",
    "            \"span_end_probs\": span_end_probs,\n",
    "            \"best_span\": best_span\n",
    "        }\n",
    "        if span_start is not None:\n",
    "            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))\n",
    "            self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))\n",
    "            self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))\n",
    "            output_dict[\"loss\"] = -expected_reward\n",
    "            \n",
    "        if metadata is not None:\n",
    "            output_dict['best_span_str'] = []\n",
    "            for i in range(batch_size):\n",
    "                passage_str = metadata[i]['original_passage']\n",
    "                offsets = metadata[i]['token_offsets']\n",
    "                predicted_span = tuple(best_span[i].data.cpu().numpy())\n",
    "                start_offset = offsets[predicted_span[0]][0]\n",
    "                end_offset = offsets[predicted_span[1]][1]\n",
    "                best_span_string = passage_str[start_offset:end_offset]\n",
    "                output_dict['best_span_str'].append(best_span_string)\n",
    "                answer_texts = metadata[i].get('answer_texts', [])\n",
    "                if answer_texts:\n",
    "                    self._squad_metrics(best_span_string, answer_texts)\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        exact_match, f1_score = self._squad_metrics.get_metric(reset)\n",
    "        return {\n",
    "                'start_acc': self._span_start_accuracy.get_metric(reset),\n",
    "                'end_acc': self._span_end_accuracy.get_metric(reset),\n",
    "                'span_acc': self._span_accuracy.get_metric(reset),\n",
    "                'em': exact_match,\n",
    "                'f1': f1_score,\n",
    "                }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_span(span_start_logits: Variable, span_end_logits: Variable) -> Variable:\n",
    "        if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n",
    "            raise ValueError(\"Input shapes must be (batch_size, passage_length)\")\n",
    "        batch_size, passage_length = span_start_logits.size()\n",
    "        max_span_log_prob = [-1e20] * batch_size\n",
    "        span_start_argmax = [0] * batch_size\n",
    "        best_word_span = Variable(span_start_logits.data.new()\n",
    "                                  .resize_(batch_size, 2).fill_(0)).long()\n",
    "\n",
    "        span_start_logits = span_start_logits.data.cpu().numpy()\n",
    "        span_end_logits = span_end_logits.data.cpu().numpy()\n",
    "\n",
    "        for b in range(batch_size):  # pylint: disable=invalid-name\n",
    "            for j in range(passage_length):\n",
    "                val1 = span_start_logits[b, span_start_argmax[b]]\n",
    "                if val1 < span_start_logits[b, j]:\n",
    "                    span_start_argmax[b] = j\n",
    "                    val1 = span_start_logits[b, j]\n",
    "\n",
    "                val2 = span_end_logits[b, j]\n",
    "\n",
    "                if val1 + val2 > max_span_log_prob[b]:\n",
    "                    best_word_span[b, 0] = span_start_argmax[b]\n",
    "                    best_word_span[b, 1] = j\n",
    "                    max_span_log_prob[b] = val1 + val2\n",
    "        return best_word_span\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BidirectionalAttentionFlow':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
    "        num_highway_layers = params.pop(\"num_highway_layers\")\n",
    "        state_controller = Seq2SeqEncoder.from_params(params.pop(\"state_controller\"))\n",
    "        phrase_layer = Seq2SeqEncoder.from_params(params.pop(\"phrase_layer\"))\n",
    "        similarity_function = SimilarityFunction.from_params(params.pop(\"similarity_function\"))\n",
    "        modeling_layer = Seq2SeqEncoder.from_params(params.pop(\"modeling_layer\"))\n",
    "        dropout = params.pop('dropout', 0.2)\n",
    "\n",
    "        # TODO: Remove the following when fully deprecated\n",
    "        evaluation_json_file = params.pop('evaluation_json_file', None)\n",
    "        if evaluation_json_file is not None:\n",
    "            logger.warning(\"the 'evaluation_json_file' model parameter is deprecated, please remove\")\n",
    "\n",
    "        init_params = params.pop('initializer', None)\n",
    "        reg_params = params.pop('regularizer', None)\n",
    "        initializer = (InitializerApplicator.from_params(init_params)\n",
    "                       if init_params is not None\n",
    "                       else InitializerApplicator())\n",
    "        regularizer = RegularizerApplicator.from_params(reg_params) if reg_params is not None else None\n",
    "\n",
    "        mask_lstms = params.pop('mask_lstms', True)\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   num_highway_layers=num_highway_layers,\n",
    "                   state_controller=state_controller,\n",
    "                   phrase_layer=phrase_layer,\n",
    "                   attention_similarity_function=similarity_function,\n",
    "                   modeling_layer=modeling_layer,\n",
    "                   dropout=dropout,\n",
    "                   mask_lstms=mask_lstms,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import percache\n",
    "\n",
    "from allennlp.common.checks import ensure_pythonhashseed_set\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.tee_logger import TeeLogger\n",
    "#from allennlp.common.util import prepare_environment\n",
    "from allennlp.data import Dataset, Vocabulary\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.archival import archive_model\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from typing import Any, Callable, Dict, List, TypeVar, Union\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from allennlp.common.checks import log_pytorch_version_info\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "JsonDict = Dict[str, Any] # pylint: disable=invalid-name\n",
    "\n",
    "def prepare_environment(params: Union[Params, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Sets random seeds for reproducible experiments. This may not work as expected\n",
    "    if you use this from within a python project in which you have already imported Pytorch.\n",
    "    If you use the scripts/run_model.py entry point to training models with this library,\n",
    "    your experiments should be reasonably reproducible. If you are using this from your own\n",
    "    project, you will want to call this function before importing Pytorch. Complete determinism\n",
    "    is very difficult to achieve with libraries doing optimized linear algebra due to massively\n",
    "    parallel execution, which is exacerbated by using GPUs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    params: Params object or dict, required.\n",
    "        A ``Params`` object or dict holding the json parameters.\n",
    "    \"\"\"\n",
    "    seed = params.pop(\"random_seed\", 13370)\n",
    "    numpy_seed = params.pop(\"numpy_seed\", 1337)\n",
    "    torch_seed = params.pop(\"pytorch_seed\", 133)\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    if numpy_seed is not None:\n",
    "        numpy.random.seed(numpy_seed)\n",
    "    if torch_seed is not None:\n",
    "        torch.manual_seed(torch_seed)\n",
    "        # Seed all GPUs with the same seed if available.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(torch_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.commands.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialization_dir = './serialization_dir'\n",
    "cache_dir = './cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PARAM:allennlp.common.params:random_seed = 13370\n",
      "PARAM:allennlp.common.params:numpy_seed = 1337\n",
      "PARAM:allennlp.common.params:pytorch_seed = 133\n",
      "PARAM:allennlp.common.params:dataset_reader.type = squad\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.type = word\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_splitter.type = spacy\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_filter.type = pass_through\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_stemmer.type = pass_through\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.start_tokens = None\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.end_tokens = None\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.type = single_id\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.type = characters\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.byte_encoding = utf-8\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.lowercase_characters = False\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.start_tokens = [259]\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.end_tokens = [260]\n",
      "PARAM:allennlp.common.params:train_data_path = ./squad/train-v1.1.json\n",
      "INFO:__main__:Reading training data from ./squad/train-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading file at ./squad/train-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading the dataset\n",
      "100%|##########| 442/442 [00:30<00:00, 14.56it/s]\n",
      "PARAM:allennlp.common.params:validation_data_path = ./squad/dev-v1.1.json\n",
      "INFO:__main__:Reading validation data from ./squad/dev-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading file at ./squad/dev-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading the dataset\n",
      "100%|##########| 48/48 [00:03<00:00, 13.06it/s]\n",
      "PARAM:allennlp.common.params:vocabulary.directory_path = None\n",
      "PARAM:allennlp.common.params:vocabulary.min_count = 1\n",
      "PARAM:allennlp.common.params:vocabulary.max_vocab_size = None\n",
      "PARAM:allennlp.common.params:vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n",
      "100%|##########| 98169/98169 [00:43<00:00, 2237.95it/s]\n",
      "PARAM:allennlp.common.params:iterator.type = bucket\n",
      "PARAM:allennlp.common.params:iterator.sorting_keys = [['passage', 'num_tokens'], ['question', 'num_tokens']]\n",
      "PARAM:allennlp.common.params:iterator.padding_noise = 0.1\n",
      "PARAM:allennlp.common.params:iterator.biggest_batch_first = False\n",
      "PARAM:allennlp.common.params:iterator.batch_size = 40\n",
      "WARNING:root:vocabulary serialization directory ./serialization_dir/vocabulary is not empty\n",
      "PARAM:allennlp.common.params:model.type = reasonet_dev\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.type = basic\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.type = embedding\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.num_embeddings = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.embedding_dim = 100\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.pretrained_file = ./glove/glove.6B.100d.txt.gz\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.projection_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.trainable = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.padding_index = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.max_norm = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.norm_type = 2.0\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.sparse = False\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading embeddings from file\n",
      "INFO:allennlp.modules.token_embedders.embedding:Initializing pre-trained embedding layer\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.type = character_encoding\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.num_embeddings = 262\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.num_highway_layers = 2\n",
      "PARAM:allennlp.common.params:model.state_controller.type = lstm\n",
      "PARAM:allennlp.common.params:model.state_controller.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.state_controller.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.state_controller.input_size = 200\n",
      "PARAM:allennlp.common.params:model.state_controller.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.state_controller.num_layers = 1\n",
      "PARAM:allennlp.common.params:model.state_controller.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.state_controller.batch_first = True\n",
      "PARAM:allennlp.common.params:model.phrase_layer.type = l_lstm\n",
      "PARAM:allennlp.common.params:model.phrase_layer.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.phrase_layer.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.phrase_layer.input_size = 200\n",
      "PARAM:allennlp.common.params:model.phrase_layer.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.phrase_layer.num_layers = 1\n",
      "PARAM:allennlp.common.params:model.phrase_layer.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.phrase_layer.batch_first = True\n",
      "PARAM:allennlp.common.params:model.similarity_function.type = linear\n",
      "PARAM:allennlp.common.params:model.similarity_function.tensor_1_dim = 200\n",
      "PARAM:allennlp.common.params:model.similarity_function.tensor_2_dim = 200\n",
      "PARAM:allennlp.common.params:model.similarity_function.combination = x,y,x*y\n",
      "PARAM:allennlp.common.params:model.similarity_function.activation = linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PARAM:allennlp.common.params:model.modeling_layer.type = lstm\n",
      "PARAM:allennlp.common.params:model.modeling_layer.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.modeling_layer.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.modeling_layer.input_size = 800\n",
      "PARAM:allennlp.common.params:model.modeling_layer.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.modeling_layer.num_layers = 2\n",
      "PARAM:allennlp.common.params:model.modeling_layer.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.modeling_layer.batch_first = True\n",
      "PARAM:allennlp.common.params:model.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.evaluation_json_file = None\n",
      "PARAM:allennlp.common.params:model.initializer = None\n",
      "PARAM:allennlp.common.params:model.regularizer = None\n",
      "PARAM:allennlp.common.params:model.mask_lstms = True\n",
      "INFO:allennlp.nn.initializers:Initializing parameters\n",
      "INFO:allennlp.nn.initializers:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.0.bias\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.0.weight\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.1.bias\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.1.weight\n",
      "INFO:allennlp.nn.initializers:   _matrix_attention._similarity_function._bias\n",
      "INFO:allennlp.nn.initializers:   _matrix_attention._similarity_function._weight_vector\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _span_end_predictor._module.bias\n",
      "INFO:allennlp.nn.initializers:   _span_end_predictor._module.weight\n",
      "INFO:allennlp.nn.initializers:   _span_start_predictor._module.bias\n",
      "INFO:allennlp.nn.initializers:   _span_start_predictor._module.weight\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens.weight\n",
      "INFO:allennlp.nn.initializers:   termination_gate.linear.bias\n",
      "INFO:allennlp.nn.initializers:   termination_gate.linear.weight\n",
      "INFO:allennlp.data.dataset:Indexing dataset\n",
      "100%|##########| 87599/87599 [01:09<00:00, 1260.02it/s]\n",
      "INFO:allennlp.data.dataset:Indexing dataset\n",
      "100%|##########| 10570/10570 [00:09<00:00, 1071.74it/s]\n",
      "PARAM:allennlp.common.params:trainer.patience = 10\n",
      "PARAM:allennlp.common.params:trainer.validation_metric = +em\n",
      "PARAM:allennlp.common.params:trainer.num_epochs = 20\n",
      "PARAM:allennlp.common.params:trainer.cuda_device = 0\n",
      "PARAM:allennlp.common.params:trainer.grad_norm = 5.0\n",
      "PARAM:allennlp.common.params:trainer.grad_clipping = None\n",
      "PARAM:allennlp.common.params:trainer.optimizer.type = adam\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:trainer.optimizer.betas = [0.9, 0.9]\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.factor = 0.5\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.mode = max\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.patience = 2\n",
      "PARAM:allennlp.common.params:trainer.no_tqdm = True\n",
      "INFO:allennlp.training.trainer:Loading model from checkpoint.\n",
      "INFO:allennlp.training.trainer:Beginning training.\n",
      "INFO:allennlp.training.trainer:Epoch 5/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.47, end_acc: 0.35, span_acc: 0.33, em: 0.35, f1: 0.44, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 20/2190: start_acc: 0.46, end_acc: 0.49, span_acc: 0.37, em: 0.42, f1: 0.55, loss: -0.95 ||\n",
      "INFO:allennlp.training.trainer:Batch 40/2190: start_acc: 0.47, end_acc: 0.50, span_acc: 0.37, em: 0.41, f1: 0.56, loss: -0.96 ||\n",
      "INFO:allennlp.training.trainer:Batch 61/2190: start_acc: 0.47, end_acc: 0.50, span_acc: 0.37, em: 0.42, f1: 0.56, loss: -0.97 ||\n",
      "INFO:allennlp.training.trainer:Batch 81/2190: start_acc: 0.47, end_acc: 0.50, span_acc: 0.37, em: 0.41, f1: 0.56, loss: -0.96 ||\n",
      "INFO:allennlp.training.trainer:Batch 101/2190: start_acc: 0.47, end_acc: 0.50, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.97 ||\n",
      "INFO:allennlp.training.trainer:Batch 125/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 144/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 164/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 186/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.42, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 207/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 226/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 249/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 270/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 286/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 305/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 325/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 345/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 365/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 389/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 402/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 424/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 444/2190: start_acc: 0.47, end_acc: 0.51, span_acc: 0.38, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 458/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.42, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 482/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 500/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 516/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 536/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 555/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 573/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 596/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 616/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 637/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 658/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 678/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 697/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 719/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 739/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 761/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 783/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 804/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 825/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 843/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 862/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 882/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 900/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 917/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 936/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 953/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 965/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 984/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1004/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1024/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1044/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1065/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1087/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1106/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1125/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1148/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1169/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1189/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1207/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1223/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1243/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1262/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1284/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1303/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1321/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1342/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1364/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.38, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1385/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1403/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1424/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1449/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1467/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1482/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1503/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1523/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1544/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.98 ||\n",
      "INFO:allennlp.training.trainer:Batch 1571/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1590/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1611/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1630/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1649/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1668/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1688/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1708/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1726/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1745/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1768/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1792/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1811/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1835/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1854/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1872/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1889/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1905/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1927/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1946/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1967/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 1984/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2005/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2024/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2044/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2064/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2080/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2100/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2119/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.56, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2137/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2157/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 2174/2190: start_acc: 0.48, end_acc: 0.51, span_acc: 0.39, em: 0.43, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 16/265: start_acc: 0.50, end_acc: 0.55, span_acc: 0.41, em: 0.50, f1: 0.64, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 93/265: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.52, f1: 0.64, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 164/265: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.53, f1: 0.64, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 240/265: start_acc: 0.52, end_acc: 0.55, span_acc: 0.43, em: 0.52, f1: 0.64, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.478168    Validation start_acc : 0.516935 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.514526    Validation end_acc : 0.553264 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.387014    Validation span_acc : 0.429518 \n",
      "INFO:allennlp.training.trainer:Training em : 0.428019    Validation em : 0.522800 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.565405    Validation f1 : 0.637364 \n",
      "INFO:allennlp.training.trainer:Training loss : -0.988531    Validation loss : -1.063734 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 6/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.47, end_acc: 0.50, span_acc: 0.42, em: 0.50, f1: 0.60, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 17/2190: start_acc: 0.49, end_acc: 0.55, span_acc: 0.41, em: 0.46, f1: 0.60, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 40/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 58/2190: start_acc: 0.47, end_acc: 0.52, span_acc: 0.40, em: 0.44, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 73/2190: start_acc: 0.47, end_acc: 0.52, span_acc: 0.39, em: 0.44, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 93/2190: start_acc: 0.48, end_acc: 0.52, span_acc: 0.40, em: 0.44, f1: 0.57, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 116/2190: start_acc: 0.47, end_acc: 0.52, span_acc: 0.39, em: 0.44, f1: 0.57, loss: -0.99 ||\n",
      "INFO:allennlp.training.trainer:Batch 136/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.57, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 157/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.57, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 181/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.57, loss: -1.00 ||\n",
      "INFO:allennlp.training.trainer:Batch 202/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.01 ||\n",
      "INFO:allennlp.training.trainer:Batch 225/2190: start_acc: 0.48, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.01 ||\n",
      "INFO:allennlp.training.trainer:Batch 244/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.01 ||\n",
      "INFO:allennlp.training.trainer:Batch 263/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.01 ||\n",
      "INFO:allennlp.training.trainer:Batch 283/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 305/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 326/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 347/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 365/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.44, f1: 0.58, loss: -1.02 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 386/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 405/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 422/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 443/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 463/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 482/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 501/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 525/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 546/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 568/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 588/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 605/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 623/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 645/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 663/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 679/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 699/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 718/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 739/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 757/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 775/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 798/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 818/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 836/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 858/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 876/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 892/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 914/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 937/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 956/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 975/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 994/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1006/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1027/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1046/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1062/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1083/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.40, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1104/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1122/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1146/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1169/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1190/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1210/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1232/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1252/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1273/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1294/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1314/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1334/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1353/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1374/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1395/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1413/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1432/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1454/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1476/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1494/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1515/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1529/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1545/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1565/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1586/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1608/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1628/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1649/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1669/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1688/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.58, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1708/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1728/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1748/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1769/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1785/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1806/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1828/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1848/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1868/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1890/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1908/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1927/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 1944/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1966/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 1986/2190: start_acc: 0.49, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 2006/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2018/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2039/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2057/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2075/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 2093/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 2111/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2132/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 2152/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 2174/2190: start_acc: 0.50, end_acc: 0.53, span_acc: 0.41, em: 0.45, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 1/265: start_acc: 0.72, end_acc: 0.68, span_acc: 0.62, em: 0.78, f1: 0.82, loss: -1.40 ||\n",
      "INFO:allennlp.training.trainer:Batch 69/265: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.54, f1: 0.67, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 135/265: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.55, f1: 0.66, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 204/265: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.54, f1: 0.66, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.495371    Validation start_acc : 0.527058 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.534253    Validation end_acc : 0.568401 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.408018    Validation span_acc : 0.438505 \n",
      "INFO:allennlp.training.trainer:Training em : 0.450724    Validation em : 0.541154 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.585858    Validation f1 : 0.657850 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.025347    Validation loss : -1.091601 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 7/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.55, end_acc: 0.40, span_acc: 0.38, em: 0.38, f1: 0.54, loss: -0.94 ||\n",
      "INFO:allennlp.training.trainer:Batch 22/2190: start_acc: 0.52, end_acc: 0.54, span_acc: 0.43, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 37/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 55/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 71/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.43, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 88/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 108/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 131/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 151/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.41, em: 0.46, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 173/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.41, em: 0.46, f1: 0.59, loss: -1.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 192/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 214/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.59, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 234/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 257/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 277/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 298/2190: start_acc: 0.50, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 316/2190: start_acc: 0.50, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 333/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 353/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 372/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 391/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 413/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 436/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 458/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 479/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 499/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 520/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 533/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 550/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 567/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 588/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 601/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 623/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 644/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 660/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 681/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 700/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 719/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 739/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 762/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 784/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 806/2190: start_acc: 0.51, end_acc: 0.54, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 827/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 848/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 867/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 888/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 910/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 929/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 950/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 972/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 990/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1011/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1025/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1043/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1064/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1085/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1106/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1127/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1144/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1163/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1182/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1201/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 1221/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1241/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.47, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1262/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1282/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1304/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1324/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1342/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1363/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1384/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1404/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1421/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1444/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1465/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1484/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1503/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1521/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1536/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1555/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1574/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1592/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1614/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1633/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1651/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1667/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1687/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1709/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1728/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1748/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1767/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1789/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1812/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1834/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1854/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1872/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1891/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1914/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1936/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1956/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1975/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 1995/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 2015/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 2036/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 2059/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 2075/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 2093/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 2115/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 2138/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 2157/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 2177/2190: start_acc: 0.51, end_acc: 0.55, span_acc: 0.42, em: 0.46, f1: 0.60, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 15/265: start_acc: 0.50, end_acc: 0.56, span_acc: 0.42, em: 0.52, f1: 0.64, loss: -1.06 ||\n",
      "INFO:allennlp.training.trainer:Batch 80/265: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.54, f1: 0.65, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 147/265: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.54, f1: 0.65, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 213/265: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.54, f1: 0.65, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.510074    Validation start_acc : 0.524882 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.549184    Validation end_acc : 0.566793 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.421603    Validation span_acc : 0.439735 \n",
      "INFO:allennlp.training.trainer:Training em : 0.464332    Validation em : 0.538694 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.601594    Validation f1 : 0.654380 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.055499    Validation loss : -1.087271 \n",
      "INFO:allennlp.training.trainer:Epoch 8/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.55, end_acc: 0.62, span_acc: 0.42, em: 0.45, f1: 0.64, loss: -1.20 ||\n",
      "INFO:allennlp.training.trainer:Batch 18/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.47, f1: 0.61, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 29/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.44, em: 0.47, f1: 0.60, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 50/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 69/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 87/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 102/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 120/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 141/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 162/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 178/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 200/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 219/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 236/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 256/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 278/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 302/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 324/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 343/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 359/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 380/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 400/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 420/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 437/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 457/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 477/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 501/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 518/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 537/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 555/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 573/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 595/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 617/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 637/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 660/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 682/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 704/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 729/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 749/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 772/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 793/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 815/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 837/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 858/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 877/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 897/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 918/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 941/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 966/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 985/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1006/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1027/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1043/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1063/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1085/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1107/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1130/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1149/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1167/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1187/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1207/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1228/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1247/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1267/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1288/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1301/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1322/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1344/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1367/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1386/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1404/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1424/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1449/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1473/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1493/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1511/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1523/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1545/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1563/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1582/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1604/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1625/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1645/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1664/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1679/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1701/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1719/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1740/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1759/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1776/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1799/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1822/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1845/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1867/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1887/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1909/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1930/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1950/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1970/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 1992/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2012/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2024/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2042/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2063/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2085/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2105/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2125/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2140/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2161/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 2181/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.47, f1: 0.61, loss: -1.07 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 28/265: start_acc: 0.53, end_acc: 0.58, span_acc: 0.46, em: 0.54, f1: 0.65, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 104/265: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.55, f1: 0.66, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 179/265: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.56, f1: 0.67, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 245/265: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.56, f1: 0.67, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.518465    Validation start_acc : 0.539830 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.557324    Validation end_acc : 0.583538 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.429560    Validation span_acc : 0.456291 \n",
      "INFO:allennlp.training.trainer:Training em : 0.473407    Validation em : 0.556575 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.610310    Validation f1 : 0.666953 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.072317    Validation loss : -1.120622 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 9/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.42, end_acc: 0.47, span_acc: 0.30, em: 0.35, f1: 0.49, loss: -0.89 ||\n",
      "INFO:allennlp.training.trainer:Batch 23/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.48, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 38/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.43, em: 0.47, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 57/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.42, em: 0.46, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 74/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.43, em: 0.47, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 91/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 110/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 128/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 148/2190: start_acc: 0.53, end_acc: 0.56, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 168/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 189/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 209/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 231/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 253/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 274/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 294/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 316/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 339/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 359/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 379/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 398/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 419/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 437/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 457/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 479/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 502/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 521/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 535/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 557/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 573/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 590/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 601/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 621/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 641/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 664/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 686/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 705/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 728/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 748/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 766/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 787/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 809/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 830/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 849/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 870/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 891/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 912/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 931/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 950/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 972/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 994/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1013/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1028/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1049/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1072/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1091/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1112/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1130/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1149/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1168/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1189/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1208/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1229/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1250/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1270/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1292/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1311/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1330/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1352/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1373/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1393/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1410/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1432/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1453/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1473/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1496/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1515/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1529/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1550/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1567/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1588/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1609/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1631/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1652/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1674/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1691/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1712/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1728/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1748/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1765/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1785/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1804/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1824/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1843/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1865/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1883/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1901/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1919/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1938/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1957/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1976/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 1993/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2013/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2027/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2044/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2063/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2084/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2103/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2123/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2144/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2165/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 2186/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 49/265: start_acc: 0.55, end_acc: 0.57, span_acc: 0.46, em: 0.57, f1: 0.66, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 127/265: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.55, f1: 0.66, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 202/265: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.56, f1: 0.67, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.525771    Validation start_acc : 0.541911 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.566685    Validation end_acc : 0.582687 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.437790    Validation span_acc : 0.458089 \n",
      "INFO:allennlp.training.trainer:Training em : 0.482494    Validation em : 0.561022 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.619117    Validation f1 : 0.668974 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.090139    Validation loss : -1.121508 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 10/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.70, end_acc: 0.55, span_acc: 0.55, em: 0.55, f1: 0.66, loss: -1.23 ||\n",
      "INFO:allennlp.training.trainer:Batch 21/2190: start_acc: 0.52, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 36/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.49, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 55/2190: start_acc: 0.52, end_acc: 0.56, span_acc: 0.43, em: 0.48, f1: 0.61, loss: -1.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 74/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.48, f1: 0.62, loss: -1.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 95/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.44, em: 0.49, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 114/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 134/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 156/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.62, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 180/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 200/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 220/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 242/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 264/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 285/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 303/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 325/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 344/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 365/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 385/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 406/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 424/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 445/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 464/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 485/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 504/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 523/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 537/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 556/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 581/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 604/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 620/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 639/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 662/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 685/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 704/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 724/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 745/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 765/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 786/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 806/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 825/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 845/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 867/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 885/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 905/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 923/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 943/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 963/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 978/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 998/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1017/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1030/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1049/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1068/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1093/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1113/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1135/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1156/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1175/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1196/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1210/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1229/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1250/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1269/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1288/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1309/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1328/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1350/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1368/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1385/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1406/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1426/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1443/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1462/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1483/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1502/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1515/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1537/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1553/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1572/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1591/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1613/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1634/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1653/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1673/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1693/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1714/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1733/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1755/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1774/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1795/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1815/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1836/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 1857/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1875/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1895/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1912/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1932/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1953/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1975/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1994/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2009/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2031/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 2050/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2072/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2095/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2115/2190: start_acc: 0.53, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2138/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2161/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 2181/2190: start_acc: 0.53, end_acc: 0.57, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.10 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 39/265: start_acc: 0.56, end_acc: 0.60, span_acc: 0.47, em: 0.58, f1: 0.69, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 113/265: start_acc: 0.56, end_acc: 0.59, span_acc: 0.47, em: 0.58, f1: 0.68, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 189/265: start_acc: 0.56, end_acc: 0.59, span_acc: 0.47, em: 0.58, f1: 0.68, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.531867    Validation start_acc : 0.549763 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.574539    Validation end_acc : 0.585714 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.445496    Validation span_acc : 0.466604 \n",
      "INFO:allennlp.training.trainer:Training em : 0.490211    Validation em : 0.571902 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.627349    Validation f1 : 0.675362 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.103878    Validation loss : -1.131325 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 11/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.57, end_acc: 0.57, span_acc: 0.47, em: 0.47, f1: 0.63, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 23/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.47, em: 0.53, f1: 0.65, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 39/2190: start_acc: 0.54, end_acc: 0.57, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 62/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 80/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 100/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 114/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 132/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 153/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 174/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 191/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 212/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 233/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 253/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 272/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 294/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 315/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 337/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 357/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 381/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 400/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 419/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 439/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 462/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 482/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 503/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 524/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 539/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 559/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 580/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 603/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 620/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 639/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 654/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 670/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 687/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 703/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 720/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 738/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 757/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 774/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 793/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 813/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 831/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 851/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 870/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 890/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 909/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 927/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 944/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 963/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 981/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1000/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1020/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1035/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1058/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1077/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1098/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1118/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1141/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1160/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1181/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1199/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1214/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1237/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1257/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1276/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1296/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1314/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1328/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1348/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1366/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1390/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1410/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1433/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1452/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1469/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1490/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1511/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1527/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1536/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1559/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1581/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1601/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1622/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1644/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1663/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1679/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1702/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1719/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1738/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1755/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1773/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1796/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1816/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1838/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1861/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1880/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1901/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1921/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1941/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1962/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 1978/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2002/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2022/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2032/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2052/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2074/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2094/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2113/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2131/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 2150/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2168/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 2187/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.63, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 46/265: start_acc: 0.56, end_acc: 0.59, span_acc: 0.47, em: 0.57, f1: 0.68, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 118/265: start_acc: 0.56, end_acc: 0.60, span_acc: 0.47, em: 0.57, f1: 0.69, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 192/265: start_acc: 0.56, end_acc: 0.59, span_acc: 0.48, em: 0.58, f1: 0.69, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 264/265: start_acc: 0.55, end_acc: 0.59, span_acc: 0.47, em: 0.57, f1: 0.68, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.538876    Validation start_acc : 0.552223 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.576502    Validation end_acc : 0.587701 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.450005    Validation span_acc : 0.465847 \n",
      "INFO:allennlp.training.trainer:Training em : 0.495428    Validation em : 0.570293 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.631743    Validation f1 : 0.679975 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.112965    Validation loss : -1.137300 \n",
      "INFO:allennlp.training.trainer:Epoch 12/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.68, end_acc: 0.78, span_acc: 0.62, em: 0.65, f1: 0.79, loss: -1.42 ||\n",
      "INFO:allennlp.training.trainer:Batch 20/2190: start_acc: 0.56, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 36/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 55/2190: start_acc: 0.55, end_acc: 0.57, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 75/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.44, em: 0.49, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 96/2190: start_acc: 0.54, end_acc: 0.57, span_acc: 0.43, em: 0.48, f1: 0.64, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 116/2190: start_acc: 0.54, end_acc: 0.57, span_acc: 0.44, em: 0.49, f1: 0.64, loss: -1.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 137/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.49, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 154/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 175/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 192/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 213/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 231/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 248/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 270/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 287/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 307/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 325/2190: start_acc: 0.55, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 343/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 365/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 386/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 408/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 426/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 445/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 468/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 487/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 502/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 520/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 531/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 552/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 575/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 596/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 617/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 640/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 660/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 679/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 698/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 718/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 738/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 752/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 776/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 794/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 815/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 832/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 851/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 873/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 897/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 918/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 941/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 961/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 980/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1000/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1018/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1036/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1049/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1068/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1087/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1108/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1126/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1147/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1165/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1183/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1203/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1222/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1243/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1262/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1283/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1302/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1321/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1341/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1361/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1379/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1395/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1415/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1438/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1456/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1477/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1498/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1518/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1534/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1553/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1571/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1591/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1611/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1632/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1652/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1674/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1694/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1714/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1739/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1761/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1780/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1800/2190: start_acc: 0.54, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1821/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1842/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1863/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1884/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1906/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1926/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 1944/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1963/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 1982/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2002/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2021/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2038/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2051/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2071/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2092/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2110/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2130/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2149/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2169/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 2187/2190: start_acc: 0.54, end_acc: 0.58, span_acc: 0.45, em: 0.50, f1: 0.64, loss: -1.12 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 45/265: start_acc: 0.55, end_acc: 0.60, span_acc: 0.46, em: 0.58, f1: 0.70, loss: -1.15 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 115/265: start_acc: 0.55, end_acc: 0.59, span_acc: 0.47, em: 0.57, f1: 0.68, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 183/265: start_acc: 0.55, end_acc: 0.59, span_acc: 0.47, em: 0.57, f1: 0.68, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 248/265: start_acc: 0.55, end_acc: 0.59, span_acc: 0.47, em: 0.57, f1: 0.68, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.541330    Validation start_acc : 0.550710 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.582632    Validation end_acc : 0.594418 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.454138    Validation span_acc : 0.466509 \n",
      "INFO:allennlp.training.trainer:Training em : 0.500257    Validation em : 0.573983 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.636225    Validation f1 : 0.684459 \n",
      "INFO:allennlp.training.trainer:Training loss : -1.121879    Validation loss : -1.141186 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 13/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.68, end_acc: 0.53, span_acc: 0.50, em: 0.53, f1: 0.66, loss: -1.21 ||\n",
      "INFO:allennlp.training.trainer:Batch 21/2190: start_acc: 0.57, end_acc: 0.61, span_acc: 0.48, em: 0.53, f1: 0.66, loss: -1.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 42/2190: start_acc: 0.56, end_acc: 0.60, span_acc: 0.47, em: 0.51, f1: 0.65, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 57/2190: start_acc: 0.57, end_acc: 0.61, span_acc: 0.48, em: 0.52, f1: 0.66, loss: -1.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 78/2190: start_acc: 0.57, end_acc: 0.60, span_acc: 0.48, em: 0.52, f1: 0.66, loss: -1.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 95/2190: start_acc: 0.57, end_acc: 0.60, span_acc: 0.48, em: 0.52, f1: 0.66, loss: -1.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 114/2190: start_acc: 0.56, end_acc: 0.59, span_acc: 0.47, em: 0.51, f1: 0.65, loss: -1.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 132/2190: start_acc: 0.56, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.65, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 156/2190: start_acc: 0.56, end_acc: 0.59, span_acc: 0.46, em: 0.51, f1: 0.65, loss: -1.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 176/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.65, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 194/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.65, loss: -1.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 216/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 234/2190: start_acc: 0.55, end_acc: 0.59, span_acc: 0.46, em: 0.50, f1: 0.64, loss: -1.14 ||\n"
     ]
    }
   ],
   "source": [
    "prepare_environment(params)\n",
    "\n",
    "os.makedirs(serialization_dir, exist_ok=True)\n",
    "sys.stdout = TeeLogger(os.path.join(serialization_dir, \"stdout.log\"), sys.stdout)  # type: ignore\n",
    "sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), sys.stderr)  # type: ignore\n",
    "handler = logging.FileHandler(os.path.join(serialization_dir, \"python_logging.log\"))\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n",
    "logging.getLogger().addHandler(handler)\n",
    "serialization_params = deepcopy(params).as_dict(quiet=True)\n",
    "with open(os.path.join(serialization_dir, \"model_params.json\"), \"w\") as param_file:\n",
    "    json.dump(serialization_params, param_file, indent=4)\n",
    "\n",
    "cache = percache.Cache(cache_dir)\n",
    "\n",
    "# Now we begin assembling the required parts for the Trainer.\n",
    "dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n",
    "train_data_path = params.pop('train_data_path')\n",
    "logger.info(\"Reading training data from %s\", train_data_path)\n",
    "train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "validation_data_path = params.pop('validation_data_path', None)\n",
    "if validation_data_path is not None:\n",
    "    logger.info(\"Reading validation data from %s\", validation_data_path)\n",
    "    validation_data = dataset_reader.read(validation_data_path)\n",
    "    combined_data = Dataset(train_data.instances + validation_data.instances)\n",
    "else:\n",
    "    validation_data = None\n",
    "    combined_data = train_data\n",
    "\n",
    "vocab = cache(Vocabulary.from_params)(params.pop(\"vocabulary\", {}), combined_data)\n",
    "iterator = cache(DataIterator.from_params)(params.pop(\"iterator\"))\n",
    "\n",
    "cache.close()\n",
    "\n",
    "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n",
    "\n",
    "model = Model.from_params(vocab, params.pop('model'))\n",
    "\n",
    "train_data.index_instances(vocab)\n",
    "if validation_data:\n",
    "    validation_data.index_instances(vocab)\n",
    "\n",
    "trainer_params = params.pop(\"trainer\")\n",
    "trainer = Trainer.from_params(model,\n",
    "                              serialization_dir,\n",
    "                              iterator,\n",
    "                              train_data,\n",
    "                              validation_data,\n",
    "                              trainer_params)\n",
    "params.assert_empty('base train command')\n",
    "trainer.train()\n",
    "\n",
    "# Now tar up results\n",
    "archive_model(serialization_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
