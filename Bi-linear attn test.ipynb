{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Части, специфичные для Reasonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:All works\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.info('All works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from allennlp.common import Params\n",
    "#from allennlp.modules.similarity_functions.similarity_function import SimilarityFunction\n",
    "from allennlp.modules.similarity_function import SimilarityFunction\n",
    "from allennlp.nn import Activation\n",
    "from torch.autograd import Variable\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.nn.util import weighted_sum\n",
    "import allennlp.nn.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CUDA_wrapper(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchwise_index(array_batch, index_batch):\n",
    "    assert index_batch.dim() == 1\n",
    "    assert array_batch.size(0) == index_batch.size(0)\n",
    "    index_batch_one_hot = CUDA_wrapper(\n",
    "        autograd.Variable(torch.ByteTensor(array_batch.size()).zero_(), requires_grad=False)\n",
    "    )\n",
    "    index_batch_one_hot.scatter_(1, index_batch.data.unsqueeze(-1), 1)\n",
    "    return array_batch[index_batch_one_hot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention over memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionCoefProvider(object):\n",
    "    \"\"\"\n",
    "    Attention coef provider\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 similarity_function: SimilarityFunction,\n",
    "                 normalize: bool) -> None:\n",
    "        self._similarity_function = similarity_function\n",
    "        self._attention = Attention(similarity_function, normalize=normalize)\n",
    "        super(AttentionCoefProvider, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        return self._attention(state, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionOverMemory(object):\n",
    "    \"\"\"\n",
    "    Attention over memory\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 coef_provider: AttentionCoefProvider) -> None:\n",
    "        self._coef_provider = coef_provider\n",
    "        super(AttentionOverMemory, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        attn_coefficients = self._coef_provider.forward(state, memory)\n",
    "        return weighted_sum(memory, attn_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AttentionProvider = AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateController(object):\n",
    "\n",
    "    def __init__(self, module: torch.nn.modules.RNNCell) -> None:\n",
    "        super(StateController, self).__init__()\n",
    "        self._module = module\n",
    "        self.hidden_state = None\n",
    "        try:\n",
    "            if not self._module.batch_first:\n",
    "                raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._module.input_size\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._module.hidden_size\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.Tensor,\n",
    "                hidden_state: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        if hidden_state is None:\n",
    "            if not self.hidden_state is None:\n",
    "                hidden_state = self.hidden_state\n",
    "            else:\n",
    "                raise ConfigurationError(\"Hidden state must be specified!\")\n",
    "                \n",
    "        self.hidden_state = self._module(inputs, hidden_state)\n",
    "        return self.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_controller = StateController(torch.nn.GRUCell(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TerminationGate(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim: int) -> None:\n",
    "        super(TerminationGate, self).__init__()\n",
    "        self._hiden_dim = hidden_dim\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, hidden_state: torch.Tensor) -> None:\n",
    "        tensor_output = util.last_dim_softmax(self.linear(hidden_state))\n",
    "        last_dim = tensor_output.dim() - 1\n",
    "        return torch.chunk(tensor_output, 2, last_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasonet inner controller logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReasoningProcess(object):\n",
    "    \n",
    "    def __init__(self, timesteps:int,\n",
    "                 attention_provider: AttentionProvider,\n",
    "                 state_controller: StateController) -> None:\n",
    "        \n",
    "        self._timesteps = timesteps\n",
    "        self._attention_provider = attention_provider\n",
    "        self._state_controller = state_controller\n",
    "        \n",
    "    def forward(self, initial_hidden_state: torch.Tensor,\n",
    "                memory: torch.Tensor):\n",
    "        \n",
    "        # use initial_hidden state to perform first state of computations\n",
    "        attn = self._attention_provider.forward(initial_hidden_state, memory)\n",
    "        hidden_state = self._state_controller.forward(attn, initial_hidden_state)\n",
    "        \n",
    "        if self._timesteps > 1:\n",
    "            for i in range(self._timesteps-1):\n",
    "                attn = self._attention_provider.forward(hidden_state, memory)\n",
    "                hidden_state = self._state_controller.forward(attn)\n",
    "                \n",
    "        return hidden_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReasoningProcessStep(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 attention_provider: AttentionProvider,\n",
    "                 state_controller: StateController) -> None:\n",
    "        \n",
    "        self._attention_provider = attention_provider\n",
    "        self._state_controller = state_controller\n",
    "        \n",
    "    def forward(self, initial_hidden_state: torch.Tensor,\n",
    "                memory: torch.Tensor):\n",
    "        \n",
    "        # use initial_hidden state to perform first state of computations\n",
    "        attn = self._attention_provider.forward(initial_hidden_state, memory)\n",
    "        hidden_state = self._state_controller.forward(attn, initial_hidden_state)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reasoner = ReasoningProcess(5, \n",
    "                            AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True)),\n",
    "                            StateController(torch.nn.GRUCell(300, 300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2SeqWrapper, that returns (all_states, last_state) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n",
    "from allennlp.nn.util import sort_batch_by_length, get_lengths_from_binary_sequence_mask\n",
    "\n",
    "\n",
    "class PytorchLastStateSeq2SeqWrapper(Seq2SeqEncoder):\n",
    "    \"\"\"\n",
    "    Pytorch's RNNs have two outputs: the hidden state for every time step, and the hidden state at\n",
    "    the last time step for every layer.  We just want the first one as a single output.  This\n",
    "    wrapper pulls out that output, and adds a :func:`get_output_dim` method, which is useful if you\n",
    "    want to, e.g., define a linear + softmax layer on top of this to get some distribution over a\n",
    "    set of labels.  The linear layer needs to know its input dimension before it is called, and you\n",
    "    can get that from ``get_output_dim``.\n",
    "\n",
    "    In order to be wrapped with this wrapper, a class must have the following members:\n",
    "\n",
    "        - ``self.input_size: int``\n",
    "        - ``self.hidden_size: int``\n",
    "        - ``def forward(inputs: PackedSequence, hidden_state: torch.autograd.Variable) ->\n",
    "          Tuple[PackedSequence, torch.autograd.Variable]``.\n",
    "        - ``self.bidirectional: bool`` (optional)\n",
    "\n",
    "    This is what pytorch's RNN's look like - just make sure your class looks like those, and it\n",
    "    should work.\n",
    "\n",
    "    Note that we *require* you to pass sequence lengths when you call this module, to avoid subtle\n",
    "    bugs around masking.  If you already have a ``PackedSequence`` you can pass ``None`` as the\n",
    "    second parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, module: torch.nn.modules.RNNBase) -> None:\n",
    "        super(PytorchLastStateSeq2SeqWrapper, self).__init__()\n",
    "        self._module = module\n",
    "        try:\n",
    "            if not self._module.batch_first:\n",
    "                raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._module.input_size\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        try:\n",
    "            is_bidirectional = self._module.bidirectional\n",
    "        except AttributeError:\n",
    "            is_bidirectional = False\n",
    "        return self._module.hidden_size * (2 if is_bidirectional else 1)\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                hidden_state: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        if mask is None:\n",
    "            return self._module(inputs, hidden_state)[0]\n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices = sort_batch_by_length(inputs,\n",
    "                                                                                           sequence_lengths)\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs,\n",
    "                                                     sorted_sequence_lengths.data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "\n",
    "        # Actually call the module on the sorted PackedSequence.\n",
    "        packed_sequence_output, state = self._module(packed_sequence_input, hidden_state)\n",
    "\n",
    "        # Deal with the fact the LSTM state is a tuple of (state, memory).\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        # Restore the original indices and return the final state of the\n",
    "        # top layer. Pytorch's recurrent layers return state in the form\n",
    "        # (num_layers * num_directions, batch_size, hidden_size) regardless\n",
    "        # of the 'batch_first' flag, so we transpose, extract the relevant\n",
    "        # layer state (both forward and backward if using bidirectional layers)\n",
    "        # and return them as a single (batch_size, self.get_output_dim()) tensor.\n",
    "\n",
    "        # now of shape: (batch_size, num_layers * num_directions, hidden_size).\n",
    "        unsorted_state = state.transpose(0, 1).index_select(0, restoration_indices)\n",
    "\n",
    "        # Extract the last hidden vector, including both forward and backward states\n",
    "        # if the cell is bidirectional. Then reshape by concatenation (in the case\n",
    "        # we have bidirectional states) or just squash the 1st dimension in the non-\n",
    "        # bidirectional case. Return tensor has shape (batch_size, hidden_size * num_directions).\n",
    "        try:\n",
    "            last_state_index = 2 if self._module.bidirectional else 1\n",
    "        except AttributeError:\n",
    "            last_state_index = 1\n",
    "        last_layer_state = unsorted_state[:, -last_state_index:, :]\n",
    "\n",
    "        unpacked_sequence_tensor, _ = pad_packed_sequence(packed_sequence_output, batch_first=True)\n",
    "        # Restore the original indices and return the sequence.\n",
    "        return unpacked_sequence_tensor.index_select(0, restoration_indices), last_layer_state.contiguous().view([-1, self.get_output_dim()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.modules.augmented_lstm import AugmentedLstm\n",
    "#from allennlp.modules.seq2seq_encoders.intra_sentence_attention import IntraSentenceAttentionEncoder\n",
    "from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import PytorchSeq2SeqWrapper\n",
    "#from allennlp.modules.seq2seq_encoders.pytorch_last_state_seq2seq_wrapper import PytorchLastStateSeq2SeqWrapper\n",
    "from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n",
    "from allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm\n",
    "\n",
    "class Seq2SeqWrapperWithLastState:\n",
    "    \"\"\"\n",
    "    For :class:`Registrable` we need to have a ``Type[Seq2SeqEncoder]`` as the value registered for each\n",
    "    key.  What that means is that we need to be able to ``__call__`` these values (as is done with\n",
    "    ``__init__`` on the class), and be able to call ``from_params()`` on the value.\n",
    "\n",
    "    In order to accomplish this, we have two options: (1) we create a ``Seq2SeqEncoder`` class for\n",
    "    all of pytorch's RNN modules individually, with our own parallel classes that we register in\n",
    "    the registry; or (2) we wrap pytorch's RNNs with something that `mimics` the required\n",
    "    API.  We've gone with the second option here.\n",
    "\n",
    "    This is a two-step approach: first, we have the :class:`PytorchSeq2SeqWrapper` class that handles\n",
    "    the interface between a pytorch RNN and our ``Seq2SeqEncoder`` API.  Our ``PytorchSeq2SeqWrapper``\n",
    "    takes an instantiated pytorch RNN and just does some interface changes.  Second, we need a way\n",
    "    to create one of these ``PytorchSeq2SeqWrappers``, with an instantiated pytorch RNN, from the\n",
    "    registry.  That's what this ``_Wrapper`` does.  The only thing this class does is instantiate\n",
    "    the pytorch RNN in a way that's compatible with ``Registrable``, then pass it off to the\n",
    "    ``PytorchSeq2SeqWrapper`` class.\n",
    "\n",
    "    When you instantiate a ``_Wrapper`` object, you give it an ``RNNBase`` subclass, which we save\n",
    "    to ``self``.  Then when called (as if we were instantiating an actual encoder with\n",
    "    ``Encoder(**params)``, or with ``Encoder.from_params(params)``), we pass those parameters\n",
    "    through to the ``RNNBase`` constructor, then pass the instantiated pytorch RNN to the\n",
    "    ``PytorchSeq2SeqWrapper``.  This lets us use this class in the registry and have everything just\n",
    "    work.\n",
    "    \"\"\"\n",
    "    PYTORCH_MODELS = [torch.nn.GRU, torch.nn.LSTM, torch.nn.RNN]\n",
    "\n",
    "    def __init__(self, module_class: Type[torch.nn.modules.RNNBase], return_last_state=False) -> None:\n",
    "        self._return_last_state = return_last_state\n",
    "        self._module_class = module_class\n",
    "\n",
    "    def __call__(self, **kwargs) -> PytorchSeq2SeqWrapper:\n",
    "        return self.from_params(Params(kwargs))\n",
    "\n",
    "    def from_params(self, params: Params) -> PytorchSeq2SeqWrapper:\n",
    "        if not params.pop('batch_first', True):\n",
    "            raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        if self._module_class in self.PYTORCH_MODELS:\n",
    "            params['batch_first'] = True\n",
    "        module = self._module_class(**params.as_dict())\n",
    "        if not self._return_last_state:\n",
    "            return PytorchSeq2SeqWrapper(module)\n",
    "        else:\n",
    "            return PytorchLastStateSeq2SeqWrapper(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Seq2SeqWrapperWithLastState at 0x7f203b16eba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq2SeqEncoder.register(\"l_lstm\")(Seq2SeqWrapperWithLastState(torch.nn.LSTM, return_last_state=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "import pyhocon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"squad\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"character_tokenizer\": {\n",
    "          \"byte_encoding\": \"utf-8\",\n",
    "          \"start_tokens\": [259],\n",
    "          \"end_tokens\": [260]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"./squad/train-v1.1.json\",\n",
    "  \"validation_data_path\": \"./squad/dev-v1.1.json\",\n",
    "  \"model\": {\n",
    "    \"type\": \"reasonet_dev\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"embedding\",\n",
    "        \"pretrained_file\": \"./glove/glove.6B.100d.txt.gz\",\n",
    "        \"embedding_dim\": 100,\n",
    "        \"trainable\": false\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"character_encoding\",\n",
    "        \"embedding\": {\n",
    "          \"num_embeddings\": 262,\n",
    "          \"embedding_dim\": 16\n",
    "        },\n",
    "        \"encoder\": {\n",
    "          \"type\": \"cnn\",\n",
    "          \"embedding_dim\": 16,\n",
    "          \"num_filters\": 100,\n",
    "          \"ngram_filter_sizes\": [5]\n",
    "        },\n",
    "        \"dropout\": 0.2\n",
    "      }\n",
    "    },\n",
    "    \"num_highway_layers\": 2,\n",
    "    \"state_controller\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"phrase_layer\": {\n",
    "      \"type\": \"l_lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"similarity_function\": {\n",
    "      \"type\": \"linear\",\n",
    "      \"combination\": \"x,y,x*y\",\n",
    "      \"tensor_1_dim\": 200,\n",
    "      \"tensor_2_dim\": 200\n",
    "    },\n",
    "    \"modeling_layer\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 800,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"dropout\": 0.2\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"sorting_keys\": [[\"passage\", \"num_tokens\"], [\"question\", \"num_tokens\"]],\n",
    "    \"batch_size\": 40\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 20,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 10,\n",
    "    \"validation_metric\": \"+em\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"learning_rate_scheduler\":  {\n",
    "      \"type\": \"reduce_on_plateau\",\n",
    "      \"factor\": 0.5,\n",
    "      \"mode\": \"max\",\n",
    "      \"patience\": 2,\n",
    "\n",
    "    },\n",
    "    \"no_tqdm\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"betas\": [0.9, 0.9]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = Params(pyhocon.ConfigFactory.parse_string(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.common.params.Params at 0x7f203b12e630>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import Highway, MatrixAttention\n",
    "from allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\n",
    "from allennlp.nn import util\n",
    "from allennlp.nn.initializers import InitializerApplicator\n",
    "from allennlp.training.regularizers import RegularizerApplicator\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "# Should import this, but can't:\n",
    "#from allennlp.training.metrics import SquadEmAndF1\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.nn.util import weighted_sum\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common import squad_eval\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "\n",
    "\n",
    "@Metric.register(\"squad\")\n",
    "class SquadEmAndF1(Metric):\n",
    "    \"\"\"\n",
    "    This :class:`Metric` takes the best span string computed by a model, along with the answer\n",
    "    strings labeled in the data, and computed exact match and F1 score using the official SQuAD\n",
    "    evaluation script.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._total_em = 0.0\n",
    "        self._total_f1 = 0.0\n",
    "        self._count = 0\n",
    "\n",
    "    @overrides\n",
    "    def __call__(self, best_span_string, answer_strings):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : ``float``\n",
    "            The value to average.\n",
    "        \"\"\"\n",
    "        exact_match = squad_eval.metric_max_over_ground_truths(\n",
    "                squad_eval.exact_match_score,\n",
    "                best_span_string,\n",
    "                answer_strings)\n",
    "        f1_score = squad_eval.metric_max_over_ground_truths(\n",
    "                squad_eval.f1_score,\n",
    "                best_span_string,\n",
    "                answer_strings)\n",
    "        self._total_em += exact_match\n",
    "        self._total_f1 += f1_score\n",
    "        self._count += 1\n",
    "\n",
    "    @overrides\n",
    "    def get_metric(self, reset: bool = False) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        Average exact match and F1 score (in that order) as computed by the official SQuAD script\n",
    "        over all inputs.\n",
    "        \"\"\"\n",
    "        exact_match = self._total_em / self._count if self._count > 0 else 0\n",
    "        f1_score = self._total_f1 / self._count if self._count > 0 else 0\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return exact_match, f1_score\n",
    "\n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self._total_em = 0.0\n",
    "        self._total_f1 = 0.0\n",
    "        self._count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"reasonet_dev\")\n",
    "class Reasonet(Model):\n",
    "    \"\"\"\n",
    "    This class implements Minjoon Seo's `Bidirectional Attention Flow model\n",
    "    <https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/7586b7cca1deba124af80609327395e613a20e9d>`_\n",
    "    for answering reading comprehension questions (ICLR 2017).\n",
    "\n",
    "    The basic layout is pretty simple: encode words as a combination of word embeddings and a\n",
    "    character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of\n",
    "    attentions to put question information into the passage word representations (this is the only\n",
    "    part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and\n",
    "    do a softmax over span start and span end.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``\n",
    "    text_field_embedder : ``TextFieldEmbedder``\n",
    "        Used to embed the ``question`` and ``passage`` ``TextFields`` we get as input to the model.\n",
    "    num_highway_layers : ``int``\n",
    "        The number of highway layers to use in between embedding the input and passing it through\n",
    "        the phrase layer.\n",
    "    phrase_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between embedding tokens\n",
    "        and doing the bidirectional attention.\n",
    "    attention_similarity_function : ``SimilarityFunction``\n",
    "        The similarity function that we will use when comparing encoded passage and question\n",
    "        representations.\n",
    "    modeling_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between the bidirectional\n",
    "        attention and predicting span start and end.\n",
    "    span_end_encoder : ``Seq2SeqEncoder``\n",
    "        The encoder that we will use to incorporate span start predictions into the passage state\n",
    "        before predicting span end.\n",
    "    dropout : ``float``, optional (default=0.2)\n",
    "        If greater than 0, we will apply dropout with this probability after all encoders (pytorch\n",
    "        LSTMs do not apply dropout to their last layer).\n",
    "    mask_lstms : ``bool``, optional (default=True)\n",
    "        If ``False``, we will skip passing the mask to the LSTM layers.  This gives a ~2x speedup,\n",
    "        with only a slight performance decrease, if any.  We haven't experimented much with this\n",
    "        yet, but have confirmed that we still get very similar performance with much faster\n",
    "        training times.  We still use the mask for all softmaxes, but avoid the shuffling that's\n",
    "        required when using masking with pytorch LSTMs.\n",
    "    evaluation_json_file : ``str``, optional\n",
    "        If given, we will load this JSON into memory and use it to compute official metrics\n",
    "        against.  We need this separately from the validation dataset, because the official metrics\n",
    "        use all of the annotations, while our dataset reader picks the most frequent one.\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 num_highway_layers: int,\n",
    "                 state_controller: Seq2SeqEncoder,\n",
    "                 phrase_layer: Seq2SeqEncoder,\n",
    "                 attention_similarity_function: SimilarityFunction,\n",
    "                 modeling_layer: Seq2SeqEncoder,\n",
    "                 dropout: float = 0.2,\n",
    "                 mask_lstms: bool = True,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None,\n",
    "                 max_timesteps: int = 5) -> None:\n",
    "        #super(Reasonet, self).__init__(vocab, regularizer)\n",
    "        super(Reasonet, self).__init__(vocab)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._highway_layer = TimeDistributed(Highway(text_field_embedder.get_output_dim(),\n",
    "                                                      num_highway_layers))\n",
    "\n",
    "        self._state_controller = state_controller\n",
    "        self._phrase_layer = phrase_layer\n",
    "        self._matrix_attention = MatrixAttention(attention_similarity_function)\n",
    "        self._modeling_layer = modeling_layer\n",
    "\n",
    "        encoding_dim = phrase_layer.get_output_dim()\n",
    "        modeling_dim = modeling_layer.get_output_dim()\n",
    "        state_controller_dim = modeling_dim\n",
    "\n",
    "        similarity_function = CUDA_wrapper(BilinearSimilarity(state_controller_dim, modeling_dim))\n",
    "        state_rnn = CUDA_wrapper(torch.nn.GRUCell(state_controller_dim, state_controller_dim))\n",
    "\n",
    "        self.termination_gate = CUDA_wrapper(TerminationGate(state_controller_dim))\n",
    "\n",
    "        coef_provider = AttentionCoefProvider(similarity_function, True)\n",
    "\n",
    "        self.reasoner_step = ReasoningProcessStep(\n",
    "            AttentionOverMemory(coef_provider),\n",
    "            StateController(state_rnn)\n",
    "        )\n",
    "        self.max_timesteps = max_timesteps\n",
    "        \n",
    "        span_start_input_dim = 2*modeling_dim\n",
    "        self._span_start_predictor = TimeDistributed(torch.nn.Linear(span_start_input_dim, 1))\n",
    "\n",
    "        span_end_input_dim = 2*modeling_dim\n",
    "        self._span_end_predictor = TimeDistributed(torch.nn.Linear(span_end_input_dim, 1))\n",
    "\n",
    "        self._span_start_accuracy = CategoricalAccuracy()\n",
    "        self._span_end_accuracy = CategoricalAccuracy()\n",
    "        self._span_accuracy = BooleanAccuracy()\n",
    "        self._squad_metrics = SquadEmAndF1()\n",
    "        if dropout > 0:\n",
    "            self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self._dropout = lambda x: x\n",
    "        self._mask_lstms = mask_lstms\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(self,  # type: ignore\n",
    "                question: Dict[str, torch.LongTensor],\n",
    "                passage: Dict[str, torch.LongTensor],\n",
    "                span_start: torch.IntTensor = None,\n",
    "                span_end: torch.IntTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.\n",
    "        passage : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.  The model assumes that this passage contains the answer to the\n",
    "            question, and predicts the beginning and ending positions of the answer within the\n",
    "            passage.\n",
    "        span_start : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            beginning position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        span_end : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            ending position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        metadata : ``List[Dict[str, Any]]``, optional\n",
    "            If present, this should contain the question ID, original passage text, and token\n",
    "            offsets into the passage for each instance in the batch.  We use this for computing\n",
    "            official metrics using the official SQuAD evaluation script.  The length of this list\n",
    "            should be the batch size, and each dictionary should have the keys ``id``,\n",
    "            ``original_passage``, and ``token_offsets``.  If you only want the best span string and\n",
    "            don't care about official metrics, you can omit the ``id`` key.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        span_start_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span start position.\n",
    "        span_start_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_start_logits)``.\n",
    "        span_end_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span end position (inclusive).\n",
    "        span_end_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_end_logits)``.\n",
    "        best_span : torch.IntTensor\n",
    "            The result of a constrained inference over ``span_start_logits`` and\n",
    "            ``span_end_logits`` to find the most probable span.  Shape is ``(batch_size, 2)``.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        best_span_str : List[str]\n",
    "            If sufficient metadata was provided for the instances in the batch, we also return the\n",
    "            string from the original passage that the model thinks is the best answer to the\n",
    "            question.\n",
    "        \"\"\"\n",
    "        embedded_question = self._highway_layer(self._text_field_embedder(question))\n",
    "        embedded_passage = self._highway_layer(self._text_field_embedder(passage))\n",
    "        batch_size = embedded_question.size(0)\n",
    "        passage_length = embedded_passage.size(1)\n",
    "        question_mask = util.get_text_field_mask(question).float()\n",
    "        passage_mask = util.get_text_field_mask(passage).float()\n",
    "        question_lstm_mask = question_mask if self._mask_lstms else None\n",
    "        passage_lstm_mask = passage_mask if self._mask_lstms else None\n",
    "\n",
    "        # We use question_last_state to initialize state controller\n",
    "        question_encoding, question_last_state = self._phrase_layer(embedded_question, question_lstm_mask)\n",
    "        passage_encoding, _ =  self._phrase_layer(embedded_passage, passage_lstm_mask)\n",
    "\n",
    "        encoded_question = self._dropout(question_encoding)\n",
    "        encoded_passage = self._dropout(passage_encoding)\n",
    "        encoding_dim = encoded_question.size(-1)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_attention = util.last_dim_softmax(passage_question_similarity, question_mask)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n",
    "\n",
    "        # We replace masked values with something really negative here, so they don't affect the\n",
    "        # max below.\n",
    "        masked_similarity = util.replace_masked_values(passage_question_similarity,\n",
    "                                                       question_mask.unsqueeze(1),\n",
    "                                                       -1e7)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_similarity = masked_similarity.max(dim=-1)[0].squeeze(-1)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_attention = util.masked_softmax(question_passage_similarity, passage_mask)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        question_passage_vector = util.weighted_sum(encoded_passage, question_passage_attention)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        tiled_question_passage_vector = question_passage_vector.unsqueeze(1).expand(batch_size,\n",
    "                                                                                    passage_length,\n",
    "                                                                                    encoding_dim)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, encoding_dim * 4)\n",
    "        final_merged_passage = torch.cat([encoded_passage,\n",
    "                                          passage_question_vectors,\n",
    "                                          encoded_passage * passage_question_vectors,\n",
    "                                          encoded_passage * tiled_question_passage_vector],\n",
    "                                         dim=-1)\n",
    "\n",
    "        modeled_passage = self._dropout(self._modeling_layer(final_merged_passage, passage_lstm_mask))\n",
    "        modeling_dim = modeled_passage.size(-1)\n",
    "\n",
    "        # !!! modelled passage = M\n",
    "        M = modeled_passage\n",
    "        \n",
    "        expected_reward = 0\n",
    "        reasoner_last_state = question_last_state\n",
    "        proceed_until_now_prob = 1.\n",
    "        for step in range(self.max_timesteps):\n",
    "            reasoner_last_state = self.reasoner_step.forward(reasoner_last_state, M)\n",
    "            proceed_prob, stop_prob = self.termination_gate.forward(reasoner_last_state)\n",
    "\n",
    "            # Shape: (batch_size, passage_length, modeling_dim)\n",
    "            tiled_reasoner_last_state = reasoner_last_state.unsqueeze(1).expand(\n",
    "                batch_size, passage_length, modeling_dim\n",
    "            )\n",
    "\n",
    "            # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))\n",
    "            answer_ready_representation = self._dropout(\n",
    "                torch.cat([modeled_passage, modeled_passage * tiled_reasoner_last_state], dim=-1)\n",
    "            )\n",
    "\n",
    "            # ! Start prediction\n",
    "\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_start_logits = self._span_start_predictor(answer_ready_representation).squeeze(-1)\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_start_probs = util.masked_softmax(span_start_logits, passage_mask)\n",
    "\n",
    "            # ! End prediction\n",
    "\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_end_logits = self._span_end_predictor(answer_ready_representation).squeeze(-1)\n",
    "            # Shape: (batch_size, passage_length)\n",
    "            span_end_probs = util.masked_softmax(span_start_logits, passage_mask)\n",
    "\n",
    "            span_start_logits = util.replace_masked_values(span_start_logits, passage_mask, -1e7)\n",
    "            span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)\n",
    "            if span_start is not None:\n",
    "                #loss = nll_loss(util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))\n",
    "                #loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))\n",
    "                reward = batchwise_index(util.masked_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))\n",
    "                reward += batchwise_index(util.masked_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))\n",
    "                expected_reward += torch.mean(reward * stop_prob * proceed_until_now_prob)\n",
    "                #expected_reward -= loss * stop_prob * proceed_until_now_prob\n",
    "            proceed_until_now_prob = proceed_until_now_prob * proceed_prob\n",
    "                \n",
    "        best_span = self._get_best_span(span_start_logits, span_end_logits)\n",
    "\n",
    "        output_dict = {\n",
    "            \"span_start_logits\": span_start_logits,\n",
    "            \"span_start_probs\": span_start_probs,\n",
    "            \"span_end_logits\": span_end_logits,\n",
    "            \"span_end_probs\": span_end_probs,\n",
    "            \"best_span\": best_span\n",
    "        }\n",
    "        if span_start is not None:\n",
    "            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))\n",
    "            self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))\n",
    "            self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))\n",
    "            output_dict[\"loss\"] = -expected_reward\n",
    "            \n",
    "        if metadata is not None:\n",
    "            output_dict['best_span_str'] = []\n",
    "            for i in range(batch_size):\n",
    "                passage_str = metadata[i]['original_passage']\n",
    "                offsets = metadata[i]['token_offsets']\n",
    "                predicted_span = tuple(best_span[i].data.cpu().numpy())\n",
    "                start_offset = offsets[predicted_span[0]][0]\n",
    "                end_offset = offsets[predicted_span[1]][1]\n",
    "                best_span_string = passage_str[start_offset:end_offset]\n",
    "                output_dict['best_span_str'].append(best_span_string)\n",
    "                answer_texts = metadata[i].get('answer_texts', [])\n",
    "                if answer_texts:\n",
    "                    self._squad_metrics(best_span_string, answer_texts)\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        exact_match, f1_score = self._squad_metrics.get_metric(reset)\n",
    "        return {\n",
    "                'start_acc': self._span_start_accuracy.get_metric(reset),\n",
    "                'end_acc': self._span_end_accuracy.get_metric(reset),\n",
    "                'span_acc': self._span_accuracy.get_metric(reset),\n",
    "                'em': exact_match,\n",
    "                'f1': f1_score,\n",
    "                }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_span(span_start_logits: Variable, span_end_logits: Variable) -> Variable:\n",
    "        if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n",
    "            raise ValueError(\"Input shapes must be (batch_size, passage_length)\")\n",
    "        batch_size, passage_length = span_start_logits.size()\n",
    "        max_span_log_prob = [-1e20] * batch_size\n",
    "        span_start_argmax = [0] * batch_size\n",
    "        best_word_span = Variable(span_start_logits.data.new()\n",
    "                                  .resize_(batch_size, 2).fill_(0)).long()\n",
    "\n",
    "        span_start_logits = span_start_logits.data.cpu().numpy()\n",
    "        span_end_logits = span_end_logits.data.cpu().numpy()\n",
    "\n",
    "        for b in range(batch_size):  # pylint: disable=invalid-name\n",
    "            for j in range(passage_length):\n",
    "                val1 = span_start_logits[b, span_start_argmax[b]]\n",
    "                if val1 < span_start_logits[b, j]:\n",
    "                    span_start_argmax[b] = j\n",
    "                    val1 = span_start_logits[b, j]\n",
    "\n",
    "                val2 = span_end_logits[b, j]\n",
    "\n",
    "                if val1 + val2 > max_span_log_prob[b]:\n",
    "                    best_word_span[b, 0] = span_start_argmax[b]\n",
    "                    best_word_span[b, 1] = j\n",
    "                    max_span_log_prob[b] = val1 + val2\n",
    "        return best_word_span\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BidirectionalAttentionFlow':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
    "        num_highway_layers = params.pop(\"num_highway_layers\")\n",
    "        state_controller = Seq2SeqEncoder.from_params(params.pop(\"state_controller\"))\n",
    "        phrase_layer = Seq2SeqEncoder.from_params(params.pop(\"phrase_layer\"))\n",
    "        similarity_function = SimilarityFunction.from_params(params.pop(\"similarity_function\"))\n",
    "        modeling_layer = Seq2SeqEncoder.from_params(params.pop(\"modeling_layer\"))\n",
    "        dropout = params.pop('dropout', 0.2)\n",
    "\n",
    "        # TODO: Remove the following when fully deprecated\n",
    "        evaluation_json_file = params.pop('evaluation_json_file', None)\n",
    "        if evaluation_json_file is not None:\n",
    "            logger.warning(\"the 'evaluation_json_file' model parameter is deprecated, please remove\")\n",
    "\n",
    "        init_params = params.pop('initializer', None)\n",
    "        reg_params = params.pop('regularizer', None)\n",
    "        initializer = (InitializerApplicator.from_params(init_params)\n",
    "                       if init_params is not None\n",
    "                       else InitializerApplicator())\n",
    "        regularizer = RegularizerApplicator.from_params(reg_params) if reg_params is not None else None\n",
    "\n",
    "        mask_lstms = params.pop('mask_lstms', True)\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   num_highway_layers=num_highway_layers,\n",
    "                   state_controller=state_controller,\n",
    "                   phrase_layer=phrase_layer,\n",
    "                   attention_similarity_function=similarity_function,\n",
    "                   modeling_layer=modeling_layer,\n",
    "                   dropout=dropout,\n",
    "                   mask_lstms=mask_lstms,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import percache\n",
    "\n",
    "from allennlp.common.checks import ensure_pythonhashseed_set\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.tee_logger import TeeLogger\n",
    "#from allennlp.common.util import prepare_environment\n",
    "from allennlp.data import Dataset, Vocabulary\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.archival import archive_model\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from typing import Any, Callable, Dict, List, TypeVar, Union\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from allennlp.common.checks import log_pytorch_version_info\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "JsonDict = Dict[str, Any] # pylint: disable=invalid-name\n",
    "\n",
    "def prepare_environment(params: Union[Params, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Sets random seeds for reproducible experiments. This may not work as expected\n",
    "    if you use this from within a python project in which you have already imported Pytorch.\n",
    "    If you use the scripts/run_model.py entry point to training models with this library,\n",
    "    your experiments should be reasonably reproducible. If you are using this from your own\n",
    "    project, you will want to call this function before importing Pytorch. Complete determinism\n",
    "    is very difficult to achieve with libraries doing optimized linear algebra due to massively\n",
    "    parallel execution, which is exacerbated by using GPUs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    params: Params object or dict, required.\n",
    "        A ``Params`` object or dict holding the json parameters.\n",
    "    \"\"\"\n",
    "    seed = params.pop(\"random_seed\", 13370)\n",
    "    numpy_seed = params.pop(\"numpy_seed\", 1337)\n",
    "    torch_seed = params.pop(\"pytorch_seed\", 133)\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    if numpy_seed is not None:\n",
    "        numpy.random.seed(numpy_seed)\n",
    "    if torch_seed is not None:\n",
    "        torch.manual_seed(torch_seed)\n",
    "        # Seed all GPUs with the same seed if available.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(torch_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.commands.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialization_dir = './serialization_dir'\n",
    "cache_dir = './cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PARAM:allennlp.common.params:random_seed = 13370\n",
      "PARAM:allennlp.common.params:numpy_seed = 1337\n",
      "PARAM:allennlp.common.params:pytorch_seed = 133\n",
      "PARAM:allennlp.common.params:dataset_reader.type = squad\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.type = word\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_splitter.type = spacy\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_filter.type = pass_through\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.word_stemmer.type = pass_through\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.start_tokens = None\n",
      "PARAM:allennlp.common.params:dataset_reader.tokenizer.end_tokens = None\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.type = single_id\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.type = characters\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.byte_encoding = utf-8\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.lowercase_characters = False\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.start_tokens = [259]\n",
      "PARAM:allennlp.common.params:dataset_reader.token_indexers.token_characters.character_tokenizer.end_tokens = [260]\n",
      "PARAM:allennlp.common.params:train_data_path = ./squad/train-v1.1.json\n",
      "INFO:__main__:Reading training data from ./squad/train-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading file at ./squad/train-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading the dataset\n",
      "100%|##########| 442/442 [00:28<00:00, 15.59it/s]\n",
      "PARAM:allennlp.common.params:validation_data_path = ./squad/dev-v1.1.json\n",
      "INFO:__main__:Reading validation data from ./squad/dev-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading file at ./squad/dev-v1.1.json\n",
      "INFO:allennlp.data.dataset_readers.squad:Reading the dataset\n",
      "100%|##########| 48/48 [00:02<00:00, 17.92it/s]\n",
      "PARAM:allennlp.common.params:vocabulary.directory_path = None\n",
      "PARAM:allennlp.common.params:vocabulary.min_count = 1\n",
      "PARAM:allennlp.common.params:vocabulary.max_vocab_size = None\n",
      "PARAM:allennlp.common.params:vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n",
      "100%|##########| 98169/98169 [00:37<00:00, 2645.38it/s]\n",
      "PARAM:allennlp.common.params:iterator.type = bucket\n",
      "PARAM:allennlp.common.params:iterator.sorting_keys = [['passage', 'num_tokens'], ['question', 'num_tokens']]\n",
      "PARAM:allennlp.common.params:iterator.padding_noise = 0.1\n",
      "PARAM:allennlp.common.params:iterator.biggest_batch_first = False\n",
      "PARAM:allennlp.common.params:iterator.batch_size = 40\n",
      "WARNING:root:vocabulary serialization directory ./serialization_dir/vocabulary is not empty\n",
      "PARAM:allennlp.common.params:model.type = reasonet_dev\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.type = basic\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.type = embedding\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.num_embeddings = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.embedding_dim = 100\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.pretrained_file = ./glove/glove.6B.100d.txt.gz\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.projection_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.trainable = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.padding_index = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.max_norm = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.norm_type = 2.0\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.tokens.sparse = False\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading embeddings from file\n",
      "INFO:allennlp.modules.token_embedders.embedding:Initializing pre-trained embedding layer\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.type = character_encoding\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.num_embeddings = 262\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "PARAM:allennlp.common.params:model.text_field_embedder.token_characters.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.num_highway_layers = 2\n",
      "PARAM:allennlp.common.params:model.state_controller.type = lstm\n",
      "PARAM:allennlp.common.params:model.state_controller.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.state_controller.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.state_controller.input_size = 200\n",
      "PARAM:allennlp.common.params:model.state_controller.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.state_controller.num_layers = 1\n",
      "PARAM:allennlp.common.params:model.state_controller.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.state_controller.batch_first = True\n",
      "PARAM:allennlp.common.params:model.phrase_layer.type = l_lstm\n",
      "PARAM:allennlp.common.params:model.phrase_layer.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.phrase_layer.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.phrase_layer.input_size = 200\n",
      "PARAM:allennlp.common.params:model.phrase_layer.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.phrase_layer.num_layers = 1\n",
      "PARAM:allennlp.common.params:model.phrase_layer.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.phrase_layer.batch_first = True\n",
      "PARAM:allennlp.common.params:model.similarity_function.type = linear\n",
      "PARAM:allennlp.common.params:model.similarity_function.tensor_1_dim = 200\n",
      "PARAM:allennlp.common.params:model.similarity_function.tensor_2_dim = 200\n",
      "PARAM:allennlp.common.params:model.similarity_function.combination = x,y,x*y\n",
      "PARAM:allennlp.common.params:model.similarity_function.activation = linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PARAM:allennlp.common.params:model.modeling_layer.type = lstm\n",
      "PARAM:allennlp.common.params:model.modeling_layer.batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:model.modeling_layer.bidirectional = True\n",
      "PARAM:allennlp.common.params:model.modeling_layer.input_size = 800\n",
      "PARAM:allennlp.common.params:model.modeling_layer.hidden_size = 100\n",
      "PARAM:allennlp.common.params:model.modeling_layer.num_layers = 2\n",
      "PARAM:allennlp.common.params:model.modeling_layer.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.modeling_layer.batch_first = True\n",
      "PARAM:allennlp.common.params:model.dropout = 0.2\n",
      "PARAM:allennlp.common.params:model.evaluation_json_file = None\n",
      "PARAM:allennlp.common.params:model.initializer = None\n",
      "PARAM:allennlp.common.params:model.regularizer = None\n",
      "PARAM:allennlp.common.params:model.mask_lstms = True\n",
      "INFO:allennlp.nn.initializers:Initializing parameters\n",
      "INFO:allennlp.nn.initializers:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.0.bias\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.0.weight\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.1.bias\n",
      "INFO:allennlp.nn.initializers:   _highway_layer._module._layers.1.weight\n",
      "INFO:allennlp.nn.initializers:   _matrix_attention._similarity_function._bias\n",
      "INFO:allennlp.nn.initializers:   _matrix_attention._similarity_function._weight_vector\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_hh_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.bias_ih_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_hh_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l1\n",
      "INFO:allennlp.nn.initializers:   _modeling_layer._module.weight_ih_l1_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _phrase_layer._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _span_end_predictor._module.bias\n",
      "INFO:allennlp.nn.initializers:   _span_end_predictor._module.weight\n",
      "INFO:allennlp.nn.initializers:   _span_start_predictor._module.bias\n",
      "INFO:allennlp.nn.initializers:   _span_start_predictor._module.weight\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_hh_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_ih_l0\n",
      "INFO:allennlp.nn.initializers:   _state_controller._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "INFO:allennlp.nn.initializers:   _text_field_embedder.token_embedder_tokens.weight\n",
      "INFO:allennlp.nn.initializers:   termination_gate.linear.bias\n",
      "INFO:allennlp.nn.initializers:   termination_gate.linear.weight\n",
      "INFO:allennlp.data.dataset:Indexing dataset\n",
      "100%|##########| 87599/87599 [01:01<00:00, 1425.18it/s]\n",
      "INFO:allennlp.data.dataset:Indexing dataset\n",
      "100%|##########| 10570/10570 [00:05<00:00, 1860.82it/s]\n",
      "PARAM:allennlp.common.params:trainer.patience = 10\n",
      "PARAM:allennlp.common.params:trainer.validation_metric = +em\n",
      "PARAM:allennlp.common.params:trainer.num_epochs = 20\n",
      "PARAM:allennlp.common.params:trainer.cuda_device = 0\n",
      "PARAM:allennlp.common.params:trainer.grad_norm = 5.0\n",
      "PARAM:allennlp.common.params:trainer.grad_clipping = None\n",
      "PARAM:allennlp.common.params:trainer.optimizer.type = adam\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:trainer.optimizer.betas = [0.9, 0.9]\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.factor = 0.5\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.mode = max\n",
      "PARAM:allennlp.common.params:trainer.learning_rate_scheduler.patience = 2\n",
      "PARAM:allennlp.common.params:trainer.no_tqdm = True\n",
      "INFO:allennlp.training.trainer:Beginning training.\n",
      "INFO:allennlp.training.trainer:Epoch 1/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.00, end_acc: 0.00, span_acc: 0.00, em: 0.00, f1: 0.12, loss: -0.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 22/2190: start_acc: 0.02, end_acc: 0.02, span_acc: 0.00, em: 0.00, f1: 0.08, loss: -0.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 43/2190: start_acc: 0.02, end_acc: 0.02, span_acc: 0.00, em: 0.00, f1: 0.07, loss: -0.02 ||\n",
      "INFO:allennlp.training.trainer:Batch 64/2190: start_acc: 0.03, end_acc: 0.03, span_acc: 0.01, em: 0.01, f1: 0.07, loss: -0.03 ||\n",
      "INFO:allennlp.training.trainer:Batch 78/2190: start_acc: 0.03, end_acc: 0.03, span_acc: 0.01, em: 0.01, f1: 0.06, loss: -0.04 ||\n",
      "INFO:allennlp.training.trainer:Batch 99/2190: start_acc: 0.03, end_acc: 0.04, span_acc: 0.02, em: 0.02, f1: 0.07, loss: -0.05 ||\n",
      "INFO:allennlp.training.trainer:Batch 121/2190: start_acc: 0.04, end_acc: 0.05, span_acc: 0.02, em: 0.02, f1: 0.07, loss: -0.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 143/2190: start_acc: 0.04, end_acc: 0.05, span_acc: 0.02, em: 0.03, f1: 0.07, loss: -0.07 ||\n",
      "INFO:allennlp.training.trainer:Batch 164/2190: start_acc: 0.04, end_acc: 0.05, span_acc: 0.03, em: 0.03, f1: 0.07, loss: -0.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 187/2190: start_acc: 0.04, end_acc: 0.05, span_acc: 0.03, em: 0.03, f1: 0.07, loss: -0.08 ||\n",
      "INFO:allennlp.training.trainer:Batch 208/2190: start_acc: 0.05, end_acc: 0.06, span_acc: 0.03, em: 0.03, f1: 0.07, loss: -0.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 229/2190: start_acc: 0.05, end_acc: 0.06, span_acc: 0.03, em: 0.03, f1: 0.08, loss: -0.09 ||\n",
      "INFO:allennlp.training.trainer:Batch 253/2190: start_acc: 0.05, end_acc: 0.06, span_acc: 0.03, em: 0.04, f1: 0.08, loss: -0.10 ||\n",
      "INFO:allennlp.training.trainer:Batch 273/2190: start_acc: 0.05, end_acc: 0.06, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.10 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 292/2190: start_acc: 0.05, end_acc: 0.06, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 312/2190: start_acc: 0.05, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 333/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 352/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.11 ||\n",
      "INFO:allennlp.training.trainer:Batch 375/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 396/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 413/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 435/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 453/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 477/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 497/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 514/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 534/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.12 ||\n",
      "INFO:allennlp.training.trainer:Batch 552/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 571/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.08, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 594/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 606/2190: start_acc: 0.06, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 627/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 650/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 673/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 692/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 713/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 734/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 757/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.13 ||\n",
      "INFO:allennlp.training.trainer:Batch 780/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 801/2190: start_acc: 0.07, end_acc: 0.07, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 824/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 843/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 863/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 883/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 901/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 919/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 939/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 958/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 977/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 997/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1017/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.04, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1038/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1061/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1084/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1105/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1125/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1146/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.14 ||\n",
      "INFO:allennlp.training.trainer:Batch 1168/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1189/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1208/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1225/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1246/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1265/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1288/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1307/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.04, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1326/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1348/2190: start_acc: 0.07, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1370/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1390/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1408/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1433/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.09, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1457/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1472/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1495/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1514/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.15 ||\n",
      "INFO:allennlp.training.trainer:Batch 1535/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1560/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1582/2190: start_acc: 0.08, end_acc: 0.08, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1604/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1623/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1640/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1664/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1682/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1696/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1716/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1735/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1757/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1780/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1801/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1822/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1841/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1859/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1877/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1893/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.16 ||\n",
      "INFO:allennlp.training.trainer:Batch 1910/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 1931/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.05, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 1950/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 1971/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 1996/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2017/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2039/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2061/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2076/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2096/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2116/2190: start_acc: 0.08, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.10, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2135/2190: start_acc: 0.09, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.11, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2156/2190: start_acc: 0.09, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.11, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Batch 2174/2190: start_acc: 0.09, end_acc: 0.09, span_acc: 0.05, em: 0.06, f1: 0.11, loss: -0.17 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 21/265: start_acc: 0.13, end_acc: 0.16, span_acc: 0.11, em: 0.13, f1: 0.19, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 81/265: start_acc: 0.13, end_acc: 0.14, span_acc: 0.10, em: 0.12, f1: 0.18, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 161/265: start_acc: 0.13, end_acc: 0.14, span_acc: 0.10, em: 0.12, f1: 0.18, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 245/265: start_acc: 0.13, end_acc: 0.14, span_acc: 0.10, em: 0.12, f1: 0.17, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.085960    Validation start_acc : 0.127152 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.094202    Validation end_acc : 0.140303 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.054886    Validation span_acc : 0.099527 \n",
      "INFO:allennlp.training.trainer:Training em : 0.058014    Validation em : 0.116556 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.106584    Validation f1 : 0.174060 \n",
      "INFO:allennlp.training.trainer:Training loss : -0.174452    Validation loss : -0.263612 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 2/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.10, end_acc: 0.10, span_acc: 0.07, em: 0.07, f1: 0.11, loss: -0.24 ||\n",
      "INFO:allennlp.training.trainer:Batch 17/2190: start_acc: 0.12, end_acc: 0.12, span_acc: 0.09, em: 0.09, f1: 0.15, loss: -0.25 ||\n",
      "INFO:allennlp.training.trainer:Batch 41/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.09, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 59/2190: start_acc: 0.12, end_acc: 0.14, span_acc: 0.09, em: 0.09, f1: 0.16, loss: -0.26 ||\n",
      "INFO:allennlp.training.trainer:Batch 75/2190: start_acc: 0.12, end_acc: 0.14, span_acc: 0.08, em: 0.09, f1: 0.15, loss: -0.25 ||\n",
      "INFO:allennlp.training.trainer:Batch 97/2190: start_acc: 0.12, end_acc: 0.14, span_acc: 0.08, em: 0.09, f1: 0.15, loss: -0.25 ||\n",
      "INFO:allennlp.training.trainer:Batch 121/2190: start_acc: 0.12, end_acc: 0.14, span_acc: 0.09, em: 0.09, f1: 0.16, loss: -0.26 ||\n",
      "INFO:allennlp.training.trainer:Batch 141/2190: start_acc: 0.13, end_acc: 0.14, span_acc: 0.09, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 161/2190: start_acc: 0.13, end_acc: 0.14, span_acc: 0.09, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 186/2190: start_acc: 0.13, end_acc: 0.14, span_acc: 0.09, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 208/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.09, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 231/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 249/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 266/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 290/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 313/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 335/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 354/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.27 ||\n",
      "INFO:allennlp.training.trainer:Batch 374/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 397/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 415/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 435/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 456/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 475/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 495/2190: start_acc: 0.13, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 520/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 541/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 565/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 585/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 603/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 620/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.16, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 642/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.28 ||\n",
      "INFO:allennlp.training.trainer:Batch 661/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 678/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 699/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 719/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 741/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 759/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 779/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 798/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 818/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 836/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 857/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 876/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 892/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 914/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.10, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 937/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 956/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 975/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 994/2190: start_acc: 0.14, end_acc: 0.15, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 1013/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 1038/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 1055/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 1074/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.29 ||\n",
      "INFO:allennlp.training.trainer:Batch 1095/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1116/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1139/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1165/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1187/2190: start_acc: 0.14, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.17, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1209/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1232/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1253/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1274/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1296/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1318/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1337/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1357/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1379/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1399/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1416/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1437/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1459/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1480/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.10, em: 0.11, f1: 0.18, loss: -0.30 ||\n",
      "INFO:allennlp.training.trainer:Batch 1501/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1523/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1540/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1561/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1582/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1604/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1624/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1647/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1667/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1687/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1707/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1727/2190: start_acc: 0.15, end_acc: 0.16, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1747/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1768/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1783/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.11, f1: 0.18, loss: -0.31 ||\n",
      "INFO:allennlp.training.trainer:Batch 1805/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.18, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1827/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.18, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1848/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.18, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1862/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1882/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1904/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1921/2190: start_acc: 0.15, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1939/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1961/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 1983/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 2002/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 2024/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 2045/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.32 ||\n",
      "INFO:allennlp.training.trainer:Batch 2063/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2083/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2101/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2120/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2142/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2167/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Batch 2185/2190: start_acc: 0.16, end_acc: 0.17, span_acc: 0.11, em: 0.12, f1: 0.19, loss: -0.33 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 41/265: start_acc: 0.23, end_acc: 0.24, span_acc: 0.18, em: 0.22, f1: 0.29, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 108/265: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.21, f1: 0.28, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 177/265: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.21, f1: 0.29, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 221/265: start_acc: 0.22, end_acc: 0.24, span_acc: 0.18, em: 0.21, f1: 0.29, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.160413    Validation start_acc : 0.223084 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.174797    Validation end_acc : 0.244182 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.114739    Validation span_acc : 0.175024 \n",
      "INFO:allennlp.training.trainer:Training em : 0.122342    Validation em : 0.210501 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.192609    Validation f1 : 0.286565 \n",
      "INFO:allennlp.training.trainer:Training loss : -0.331010    Validation loss : -0.461053 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 3/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.38, end_acc: 0.17, span_acc: 0.15, em: 0.15, f1: 0.28, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 22/2190: start_acc: 0.21, end_acc: 0.24, span_acc: 0.16, em: 0.18, f1: 0.27, loss: -0.44 ||\n",
      "INFO:allennlp.training.trainer:Batch 43/2190: start_acc: 0.20, end_acc: 0.22, span_acc: 0.15, em: 0.17, f1: 0.25, loss: -0.42 ||\n",
      "INFO:allennlp.training.trainer:Batch 59/2190: start_acc: 0.21, end_acc: 0.23, span_acc: 0.16, em: 0.18, f1: 0.26, loss: -0.43 ||\n",
      "INFO:allennlp.training.trainer:Batch 76/2190: start_acc: 0.21, end_acc: 0.23, span_acc: 0.16, em: 0.18, f1: 0.25, loss: -0.43 ||\n",
      "INFO:allennlp.training.trainer:Batch 93/2190: start_acc: 0.21, end_acc: 0.24, span_acc: 0.16, em: 0.18, f1: 0.26, loss: -0.44 ||\n",
      "INFO:allennlp.training.trainer:Batch 114/2190: start_acc: 0.21, end_acc: 0.24, span_acc: 0.16, em: 0.18, f1: 0.26, loss: -0.44 ||\n",
      "INFO:allennlp.training.trainer:Batch 137/2190: start_acc: 0.21, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.26, loss: -0.44 ||\n",
      "INFO:allennlp.training.trainer:Batch 157/2190: start_acc: 0.21, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.26, loss: -0.44 ||\n",
      "INFO:allennlp.training.trainer:Batch 179/2190: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.26, loss: -0.45 ||\n",
      "INFO:allennlp.training.trainer:Batch 198/2190: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.26, loss: -0.45 ||\n",
      "INFO:allennlp.training.trainer:Batch 222/2190: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.27, loss: -0.45 ||\n",
      "INFO:allennlp.training.trainer:Batch 242/2190: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.27, loss: -0.45 ||\n",
      "INFO:allennlp.training.trainer:Batch 264/2190: start_acc: 0.22, end_acc: 0.24, span_acc: 0.17, em: 0.18, f1: 0.27, loss: -0.45 ||\n",
      "INFO:allennlp.training.trainer:Batch 287/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 305/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 323/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 342/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.46 ||\n",
      "INFO:allennlp.training.trainer:Batch 359/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 376/2190: start_acc: 0.22, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 399/2190: start_acc: 0.23, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 413/2190: start_acc: 0.23, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.27, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 436/2190: start_acc: 0.23, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 458/2190: start_acc: 0.23, end_acc: 0.25, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.47 ||\n",
      "INFO:allennlp.training.trainer:Batch 479/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.48 ||\n",
      "INFO:allennlp.training.trainer:Batch 500/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.48 ||\n",
      "INFO:allennlp.training.trainer:Batch 521/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.17, em: 0.19, f1: 0.28, loss: -0.48 ||\n",
      "INFO:allennlp.training.trainer:Batch 540/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.18, em: 0.19, f1: 0.28, loss: -0.48 ||\n",
      "INFO:allennlp.training.trainer:Batch 558/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.18, em: 0.19, f1: 0.28, loss: -0.48 ||\n",
      "INFO:allennlp.training.trainer:Batch 575/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.18, em: 0.19, f1: 0.28, loss: -0.49 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 594/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.18, em: 0.20, f1: 0.28, loss: -0.49 ||\n",
      "INFO:allennlp.training.trainer:Batch 612/2190: start_acc: 0.23, end_acc: 0.26, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.49 ||\n",
      "INFO:allennlp.training.trainer:Batch 633/2190: start_acc: 0.24, end_acc: 0.26, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.49 ||\n",
      "INFO:allennlp.training.trainer:Batch 651/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.49 ||\n",
      "INFO:allennlp.training.trainer:Batch 668/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.49 ||\n",
      "INFO:allennlp.training.trainer:Batch 688/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.50 ||\n",
      "INFO:allennlp.training.trainer:Batch 706/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.50 ||\n",
      "INFO:allennlp.training.trainer:Batch 727/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.50 ||\n",
      "INFO:allennlp.training.trainer:Batch 748/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.50 ||\n",
      "INFO:allennlp.training.trainer:Batch 771/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.29, loss: -0.50 ||\n",
      "INFO:allennlp.training.trainer:Batch 793/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.18, em: 0.20, f1: 0.30, loss: -0.51 ||\n",
      "INFO:allennlp.training.trainer:Batch 815/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.19, em: 0.20, f1: 0.30, loss: -0.51 ||\n",
      "INFO:allennlp.training.trainer:Batch 834/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.51 ||\n",
      "INFO:allennlp.training.trainer:Batch 855/2190: start_acc: 0.24, end_acc: 0.27, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.51 ||\n",
      "INFO:allennlp.training.trainer:Batch 876/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.51 ||\n",
      "INFO:allennlp.training.trainer:Batch 899/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.52 ||\n",
      "INFO:allennlp.training.trainer:Batch 918/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.52 ||\n",
      "INFO:allennlp.training.trainer:Batch 930/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.30, loss: -0.52 ||\n",
      "INFO:allennlp.training.trainer:Batch 951/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.52 ||\n",
      "INFO:allennlp.training.trainer:Batch 973/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 991/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 1012/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 1033/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 1053/2190: start_acc: 0.25, end_acc: 0.28, span_acc: 0.19, em: 0.21, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 1075/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.31, loss: -0.53 ||\n",
      "INFO:allennlp.training.trainer:Batch 1096/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.31, loss: -0.54 ||\n",
      "INFO:allennlp.training.trainer:Batch 1118/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.54 ||\n",
      "INFO:allennlp.training.trainer:Batch 1136/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.54 ||\n",
      "INFO:allennlp.training.trainer:Batch 1156/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.54 ||\n",
      "INFO:allennlp.training.trainer:Batch 1176/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.54 ||\n",
      "INFO:allennlp.training.trainer:Batch 1196/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1217/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1239/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1257/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1277/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1296/2190: start_acc: 0.26, end_acc: 0.29, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1314/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.55 ||\n",
      "INFO:allennlp.training.trainer:Batch 1334/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.20, em: 0.22, f1: 0.32, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1354/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.20, em: 0.23, f1: 0.33, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1374/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1396/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1412/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1427/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.56 ||\n",
      "INFO:allennlp.training.trainer:Batch 1451/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 1464/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 1483/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 1502/2190: start_acc: 0.27, end_acc: 0.30, span_acc: 0.21, em: 0.23, f1: 0.33, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 1525/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.23, f1: 0.34, loss: -0.57 ||\n",
      "INFO:allennlp.training.trainer:Batch 1539/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.23, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1560/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.23, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1578/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.23, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1595/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.23, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1618/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.24, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1637/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.21, em: 0.24, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1655/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.58 ||\n",
      "INFO:allennlp.training.trainer:Batch 1672/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1693/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1715/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1735/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1754/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.34, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1775/2190: start_acc: 0.28, end_acc: 0.31, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1796/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.59 ||\n",
      "INFO:allennlp.training.trainer:Batch 1819/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 1841/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1860/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1882/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1899/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1924/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1946/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.24, f1: 0.35, loss: -0.60 ||\n",
      "INFO:allennlp.training.trainer:Batch 1962/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.25, f1: 0.35, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 1983/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.25, f1: 0.35, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2000/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.25, f1: 0.35, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2020/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.25, f1: 0.35, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2043/2190: start_acc: 0.29, end_acc: 0.32, span_acc: 0.22, em: 0.25, f1: 0.36, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2066/2190: start_acc: 0.29, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2083/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.61 ||\n",
      "INFO:allennlp.training.trainer:Batch 2102/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.62 ||\n",
      "INFO:allennlp.training.trainer:Batch 2122/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.62 ||\n",
      "INFO:allennlp.training.trainer:Batch 2145/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.62 ||\n",
      "INFO:allennlp.training.trainer:Batch 2163/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.62 ||\n",
      "INFO:allennlp.training.trainer:Batch 2186/2190: start_acc: 0.30, end_acc: 0.33, span_acc: 0.23, em: 0.25, f1: 0.36, loss: -0.62 ||\n",
      "INFO:allennlp.training.trainer:Validating\n",
      "INFO:allennlp.training.trainer:Batch 57/265: start_acc: 0.42, end_acc: 0.45, span_acc: 0.33, em: 0.41, f1: 0.52, loss: -0.87 ||\n",
      "INFO:allennlp.training.trainer:Batch 136/265: start_acc: 0.41, end_acc: 0.44, span_acc: 0.33, em: 0.40, f1: 0.51, loss: -0.85 ||\n",
      "INFO:allennlp.training.trainer:Batch 214/265: start_acc: 0.41, end_acc: 0.45, span_acc: 0.33, em: 0.40, f1: 0.52, loss: -0.85 ||\n",
      "INFO:allennlp.training.trainer:Training start_acc : 0.299273    Validation start_acc : 0.411826 \n",
      "INFO:allennlp.training.trainer:Training end_acc : 0.330232    Validation end_acc : 0.445506 \n",
      "INFO:allennlp.training.trainer:Training span_acc : 0.229512    Validation span_acc : 0.330180 \n",
      "INFO:allennlp.training.trainer:Training em : 0.253336    Validation em : 0.402460 \n",
      "INFO:allennlp.training.trainer:Training f1 : 0.362553    Validation f1 : 0.516122 \n",
      "INFO:allennlp.training.trainer:Training loss : -0.622155    Validation loss : -0.850930 \n",
      "INFO:allennlp.training.trainer:Best validation performance so far. Copying weights to ./serialization_dir/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch 4/20\n",
      "INFO:allennlp.training.trainer:Training\n",
      "INFO:allennlp.training.trainer:Batch 1/2190: start_acc: 0.28, end_acc: 0.38, span_acc: 0.23, em: 0.25, f1: 0.40, loss: -0.68 ||\n",
      "INFO:allennlp.training.trainer:Batch 18/2190: start_acc: 0.37, end_acc: 0.39, span_acc: 0.28, em: 0.31, f1: 0.44, loss: -0.75 ||\n",
      "INFO:allennlp.training.trainer:Batch 36/2190: start_acc: 0.37, end_acc: 0.39, span_acc: 0.28, em: 0.31, f1: 0.44, loss: -0.75 ||\n",
      "INFO:allennlp.training.trainer:Batch 59/2190: start_acc: 0.37, end_acc: 0.39, span_acc: 0.28, em: 0.32, f1: 0.44, loss: -0.75 ||\n",
      "INFO:allennlp.training.trainer:Batch 78/2190: start_acc: 0.38, end_acc: 0.40, span_acc: 0.29, em: 0.33, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 95/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.76 ||\n",
      "INFO:allennlp.training.trainer:Batch 111/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.76 ||\n",
      "INFO:allennlp.training.trainer:Batch 134/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 156/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.44, loss: -0.76 ||\n",
      "INFO:allennlp.training.trainer:Batch 174/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.44, loss: -0.76 ||\n",
      "INFO:allennlp.training.trainer:Batch 196/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 215/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.44, loss: -0.76 ||\n",
      "INFO:allennlp.training.trainer:Batch 233/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 252/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 275/2190: start_acc: 0.37, end_acc: 0.40, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 298/2190: start_acc: 0.37, end_acc: 0.41, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.77 ||\n",
      "INFO:allennlp.training.trainer:Batch 319/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.78 ||\n",
      "INFO:allennlp.training.trainer:Batch 338/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.78 ||\n",
      "INFO:allennlp.training.trainer:Batch 355/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.29, em: 0.32, f1: 0.45, loss: -0.78 ||\n",
      "INFO:allennlp.training.trainer:Batch 374/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.45, loss: -0.78 ||\n",
      "INFO:allennlp.training.trainer:Batch 392/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.45, loss: -0.78 ||\n",
      "INFO:allennlp.training.trainer:Batch 412/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 428/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 448/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.29, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 466/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 488/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 501/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 520/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 538/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 555/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 573/2190: start_acc: 0.38, end_acc: 0.41, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 595/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 617/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 636/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 658/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 678/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Batch 699/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 722/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.79 ||\n",
      "INFO:allennlp.training.trainer:Batch 744/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 766/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 785/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 808/2190: start_acc: 0.38, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 830/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 852/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 873/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 892/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 912/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 932/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 957/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 973/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 993/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 1015/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.46, loss: -0.80 ||\n",
      "INFO:allennlp.training.trainer:Batch 1034/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1050/2190: start_acc: 0.39, end_acc: 0.42, span_acc: 0.30, em: 0.33, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1073/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.30, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1095/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.30, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1118/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.30, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1140/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.30, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1159/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1179/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.30, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1199/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1220/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1240/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.81 ||\n",
      "INFO:allennlp.training.trainer:Batch 1257/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1281/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1296/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1317/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1337/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1359/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1379/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1398/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1416/2190: start_acc: 0.39, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1440/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.47, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1465/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1484/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1503/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1523/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1537/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n",
      "INFO:allennlp.training.trainer:Batch 1556/2190: start_acc: 0.40, end_acc: 0.43, span_acc: 0.31, em: 0.34, f1: 0.48, loss: -0.82 ||\n"
     ]
    }
   ],
   "source": [
    "prepare_environment(params)\n",
    "\n",
    "os.makedirs(serialization_dir, exist_ok=True)\n",
    "sys.stdout = TeeLogger(os.path.join(serialization_dir, \"stdout.log\"), sys.stdout)  # type: ignore\n",
    "sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), sys.stderr)  # type: ignore\n",
    "handler = logging.FileHandler(os.path.join(serialization_dir, \"python_logging.log\"))\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n",
    "logging.getLogger().addHandler(handler)\n",
    "serialization_params = deepcopy(params).as_dict(quiet=True)\n",
    "with open(os.path.join(serialization_dir, \"model_params.json\"), \"w\") as param_file:\n",
    "    json.dump(serialization_params, param_file, indent=4)\n",
    "\n",
    "cache = percache.Cache(cache_dir)\n",
    "\n",
    "# Now we begin assembling the required parts for the Trainer.\n",
    "dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n",
    "train_data_path = params.pop('train_data_path')\n",
    "logger.info(\"Reading training data from %s\", train_data_path)\n",
    "train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "validation_data_path = params.pop('validation_data_path', None)\n",
    "if validation_data_path is not None:\n",
    "    logger.info(\"Reading validation data from %s\", validation_data_path)\n",
    "    validation_data = dataset_reader.read(validation_data_path)\n",
    "    combined_data = Dataset(train_data.instances + validation_data.instances)\n",
    "else:\n",
    "    validation_data = None\n",
    "    combined_data = train_data\n",
    "\n",
    "vocab = cache(Vocabulary.from_params)(params.pop(\"vocabulary\", {}), combined_data)\n",
    "iterator = cache(DataIterator.from_params)(params.pop(\"iterator\"))\n",
    "\n",
    "cache.close()\n",
    "\n",
    "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n",
    "\n",
    "model = Model.from_params(vocab, params.pop('model'))\n",
    "\n",
    "train_data.index_instances(vocab)\n",
    "if validation_data:\n",
    "    validation_data.index_instances(vocab)\n",
    "\n",
    "trainer_params = params.pop(\"trainer\")\n",
    "trainer = Trainer.from_params(model,\n",
    "                              serialization_dir,\n",
    "                              iterator,\n",
    "                              train_data,\n",
    "                              validation_data,\n",
    "                              trainer_params)\n",
    "params.assert_empty('base train command')\n",
    "trainer.train()\n",
    "\n",
    "# Now tar up results\n",
    "archive_model(serialization_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
