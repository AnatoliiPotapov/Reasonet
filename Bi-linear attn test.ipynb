{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Части, специфичные для Reasonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from allennlp.common import Params\n",
    "from allennlp.modules.similarity_functions.similarity_function import SimilarityFunction\n",
    "from allennlp.nn import Activation\n",
    "from torch.autograd import Variable\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.nn.util import weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention over memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionCoefProvider(object):\n",
    "    \"\"\"\n",
    "    Attention coef provider\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 similarity_function: SimilarityFunction,\n",
    "                 normalize: bool) -> None:\n",
    "        self._similarity_function = similarity_function\n",
    "        self._attention = Attention(similarity_function, normalize=normalize)\n",
    "        super(AttentionCoefProvider, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        return self._attention(state, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttentionOverMemory(object):\n",
    "    \"\"\"\n",
    "    Attention over memory\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 coef_provider: AttentionCoefProvider) -> None:\n",
    "        self._coef_provider = coef_provider\n",
    "        super(AttentionOverMemory, self).__init__()\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        attn_coefficients = self._coef_provider.forward(state, memory)\n",
    "        return weighted_sum(memory, attn_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AttentionProvider = AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(10,300).zero_())\n",
    "y = Variable(torch.Tensor(10,20,300).zero_())\n",
    "mask = Variable(torch.nn.init.constant(torch.Tensor(10,20,300),1))\n",
    "M = Variable(torch.Tensor(10,20,300).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.FloatTensor of size 10x300]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionProvider.forward(x, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateController(object):\n",
    "\n",
    "    def __init__(self, module: torch.nn.modules.RNNCell) -> None:\n",
    "        super(StateController, self).__init__()\n",
    "        self._module = module\n",
    "        self.hidden_state = None\n",
    "        try:\n",
    "            if not self._module.batch_first:\n",
    "                raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._module.input_size\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._module.hidden_size\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.Tensor,\n",
    "                hidden_state: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        if hidden_state is None:\n",
    "            if not self.hidden_state is None:\n",
    "                hidden_state = self.hidden_state\n",
    "            else:\n",
    "                raise ConfigurationError(\"Hidden state must be specified!\")\n",
    "                \n",
    "        self.hidden_state = self._module(inputs, hidden_state)\n",
    "        return self.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_state = Variable(torch.Tensor(10,300).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_controller = StateController(torch.nn.GRUCell(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TerminationGate(object):\n",
    "    \n",
    "    def __init__(self, hidden_dim:int)->None:\n",
    "        self._hiden_dim = hidden_dim\n",
    "        self.layer = torch.nn.Linear(hidden_dim)\n",
    "        \n",
    "    def forwarf(self, hidden_state:torch.Tensor)->None:\n",
    "        return self.layer(hidden_state)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasonet inner controller logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReasoningProcess(object):\n",
    "    \n",
    "    def __init__(self, timesteps:int,\n",
    "                 attention_provider: AttentionProvider,\n",
    "                 state_controller: StateController) -> None:\n",
    "        \n",
    "        self._timesteps = timesteps\n",
    "        self._attention_provider = attention_provider\n",
    "        self._state_controller = state_controller\n",
    "        \n",
    "    def forward(self, initial_hidden_state: torch.Tensor,\n",
    "                memory: torch.Tensor):\n",
    "        \n",
    "        # use initial_hidden state to perform first state of computations\n",
    "        attn = self._attention_provider.forward(initial_hidden_state, memory)\n",
    "        hidden_state = self._state_controller.forward(attn, initial_hidden_state)\n",
    "        \n",
    "        if self._timesteps > 1:\n",
    "            for i in range(self._timesteps-1):\n",
    "                attn = self._attention_provider.forward(hidden_state, memory)\n",
    "                hidden_state = self._state_controller.forward(attn)\n",
    "                \n",
    "        return hidden_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reasoner = ReasoningProcess(5, \n",
    "                            AttentionOverMemory(AttentionCoefProvider(BilinearSimilarity(300,300), True)),\n",
    "                            StateController(torch.nn.GRUCell(300, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_representation = reasoner.forward(init_state, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "          ...             ⋱             ...          \n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       " 1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "[torch.FloatTensor of size 10x300]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tiled_start_representation = start_representation.unsqueeze(1).expand(10,20,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "\n",
       "( 1 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "\n",
       "( 2 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "... \n",
       "\n",
       "( 7 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "\n",
       "( 8 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "\n",
       "( 9 ,.,.) = \n",
       "1.00000e-02 *\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "           ...             ⋱             ...          \n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "  1.0865 -4.5946  0.1054  ...   5.5446 -0.5163  8.1218\n",
       "[torch.FloatTensor of size 10x20x300]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiled_start_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "import pyhocon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"squad\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"character_tokenizer\": {\n",
    "          \"byte_encoding\": \"utf-8\",\n",
    "          \"start_tokens\": [259],\n",
    "          \"end_tokens\": [260]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"./squad/train-v1.1.json\",\n",
    "  \"validation_data_path\": \"./squad/dev-v1.1.json\",\n",
    "  \"model\": {\n",
    "    \"type\": \"reasonet_dev\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"embedding\",\n",
    "        \"pretrained_file\": \"./glove/glove.6B.100d.txt.gz\",\n",
    "        \"embedding_dim\": 100,\n",
    "        \"trainable\": false\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"character_encoding\",\n",
    "        \"embedding\": {\n",
    "          \"num_embeddings\": 262,\n",
    "          \"embedding_dim\": 16\n",
    "        },\n",
    "        \"encoder\": {\n",
    "          \"type\": \"cnn\",\n",
    "          \"embedding_dim\": 16,\n",
    "          \"num_filters\": 100,\n",
    "          \"ngram_filter_sizes\": [5]\n",
    "        },\n",
    "        \"dropout\": 0.2\n",
    "      }\n",
    "    },\n",
    "    \"num_highway_layers\": 2,\n",
    "    \"state_controller\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"phrase_layer\": {\n",
    "      \"type\": \"l_lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"similarity_function\": {\n",
    "      \"type\": \"linear\",\n",
    "      \"combination\": \"x,y,x*y\",\n",
    "      \"tensor_1_dim\": 200,\n",
    "      \"tensor_2_dim\": 200\n",
    "    },\n",
    "    \"modeling_layer\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 800,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"dropout\": 0.2\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"sorting_keys\": [[\"passage\", \"num_tokens\"], [\"question\", \"num_tokens\"]],\n",
    "    \"batch_size\": 40\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 20,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 10,\n",
    "    \"validation_metric\": \"+em\",\n",
    "    \"cuda_device\": -1,\n",
    "    \"learning_rate_scheduler\":  {\n",
    "      \"type\": \"reduce_on_plateau\",\n",
    "      \"factor\": 0.5,\n",
    "      \"mode\": \"max\",\n",
    "      \"patience\": 2,\n",
    "\n",
    "    },\n",
    "    \"no_tqdm\": true,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"betas\": [0.9, 0.9]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = Params(pyhocon.ConfigFactory.parse_string(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.common.params.Params at 0x7f156718e630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import Highway, MatrixAttention\n",
    "from allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\n",
    "from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy, SquadEmAndF1\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.modules.similarity_functions.bilinear import BilinearSimilarity\n",
    "from allennlp.nn.util import weighted_sum\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@Model.register(\"reasonet_dev\")\n",
    "class Reasonet(Model):\n",
    "    \"\"\"\n",
    "    This class implements Minjoon Seo's `Bidirectional Attention Flow model\n",
    "    <https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/7586b7cca1deba124af80609327395e613a20e9d>`_\n",
    "    for answering reading comprehension questions (ICLR 2017).\n",
    "\n",
    "    The basic layout is pretty simple: encode words as a combination of word embeddings and a\n",
    "    character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of\n",
    "    attentions to put question information into the passage word representations (this is the only\n",
    "    part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and\n",
    "    do a softmax over span start and span end.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``\n",
    "    text_field_embedder : ``TextFieldEmbedder``\n",
    "        Used to embed the ``question`` and ``passage`` ``TextFields`` we get as input to the model.\n",
    "    num_highway_layers : ``int``\n",
    "        The number of highway layers to use in between embedding the input and passing it through\n",
    "        the phrase layer.\n",
    "    phrase_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between embedding tokens\n",
    "        and doing the bidirectional attention.\n",
    "    attention_similarity_function : ``SimilarityFunction``\n",
    "        The similarity function that we will use when comparing encoded passage and question\n",
    "        representations.\n",
    "    modeling_layer : ``Seq2SeqEncoder``\n",
    "        The encoder (with its own internal stacking) that we will use in between the bidirectional\n",
    "        attention and predicting span start and end.\n",
    "    span_end_encoder : ``Seq2SeqEncoder``\n",
    "        The encoder that we will use to incorporate span start predictions into the passage state\n",
    "        before predicting span end.\n",
    "    dropout : ``float``, optional (default=0.2)\n",
    "        If greater than 0, we will apply dropout with this probability after all encoders (pytorch\n",
    "        LSTMs do not apply dropout to their last layer).\n",
    "    mask_lstms : ``bool``, optional (default=True)\n",
    "        If ``False``, we will skip passing the mask to the LSTM layers.  This gives a ~2x speedup,\n",
    "        with only a slight performance decrease, if any.  We haven't experimented much with this\n",
    "        yet, but have confirmed that we still get very similar performance with much faster\n",
    "        training times.  We still use the mask for all softmaxes, but avoid the shuffling that's\n",
    "        required when using masking with pytorch LSTMs.\n",
    "    evaluation_json_file : ``str``, optional\n",
    "        If given, we will load this JSON into memory and use it to compute official metrics\n",
    "        against.  We need this separately from the validation dataset, because the official metrics\n",
    "        use all of the annotations, while our dataset reader picks the most frequent one.\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 num_highway_layers: int,\n",
    "                 state_controller: Seq2SeqEncoder,\n",
    "                 phrase_layer: Seq2SeqEncoder,\n",
    "                 attention_similarity_function: SimilarityFunction,\n",
    "                 modeling_layer: Seq2SeqEncoder,\n",
    "                 dropout: float = 0.2,\n",
    "                 mask_lstms: bool = True,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(Reasonet, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._highway_layer = TimeDistributed(Highway(text_field_embedder.get_output_dim(),\n",
    "                                                      num_highway_layers))\n",
    "\n",
    "        self._state_controller = state_controller\n",
    "        self._phrase_layer = phrase_layer\n",
    "        self._matrix_attention = MatrixAttention(attention_similarity_function)\n",
    "        self._modeling_layer = modeling_layer\n",
    "\n",
    "        encoding_dim = phrase_layer.get_output_dim()\n",
    "        modeling_dim = modeling_layer.get_output_dim()\n",
    "        state_controller_dim = modeling_dim\n",
    "\n",
    "        similarity_function = BilinearSimilarity(state_controller_dim, modeling_dim)\n",
    "        state_rnn = torch.nn.GRUCell(state_controller_dim, state_controller_dim)\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        #print(\"Cuda detected\")\n",
    "        #similarity_function.cuda()\n",
    "        #state_rnn.cuda()\n",
    "\n",
    "        coef_provider = AttentionCoefProvider(similarity_function, True)\n",
    "\n",
    "        self.reasoner = ReasoningProcess(\n",
    "            5,\n",
    "            AttentionOverMemory(coef_provider),\n",
    "            StateController(state_rnn)\n",
    "        )\n",
    "\n",
    "        span_start_input_dim = 2*modeling_dim\n",
    "        self._span_start_predictor = TimeDistributed(torch.nn.Linear(span_start_input_dim, 1))\n",
    "\n",
    "        span_end_input_dim = 2*modeling_dim\n",
    "        self._span_end_predictor = TimeDistributed(torch.nn.Linear(span_end_input_dim, 1))\n",
    "\n",
    "        self._span_start_accuracy = CategoricalAccuracy()\n",
    "        self._span_end_accuracy = CategoricalAccuracy()\n",
    "        self._span_accuracy = BooleanAccuracy()\n",
    "        self._squad_metrics = SquadEmAndF1()\n",
    "        if dropout > 0:\n",
    "            self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self._dropout = lambda x: x\n",
    "        self._mask_lstms = mask_lstms\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(self,  # type: ignore\n",
    "                question: Dict[str, torch.LongTensor],\n",
    "                passage: Dict[str, torch.LongTensor],\n",
    "                span_start: torch.IntTensor = None,\n",
    "                span_end: torch.IntTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.\n",
    "        passage : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``.  The model assumes that this passage contains the answer to the\n",
    "            question, and predicts the beginning and ending positions of the answer within the\n",
    "            passage.\n",
    "        span_start : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            beginning position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        span_end : ``torch.IntTensor``, optional\n",
    "            From an ``IndexField``.  This is one of the things we are trying to predict - the\n",
    "            ending position of the answer with the passage.  This is an `inclusive` index.  If\n",
    "            this is given, we will compute a loss that gets included in the output dictionary.\n",
    "        metadata : ``List[Dict[str, Any]]``, optional\n",
    "            If present, this should contain the question ID, original passage text, and token\n",
    "            offsets into the passage for each instance in the batch.  We use this for computing\n",
    "            official metrics using the official SQuAD evaluation script.  The length of this list\n",
    "            should be the batch size, and each dictionary should have the keys ``id``,\n",
    "            ``original_passage``, and ``token_offsets``.  If you only want the best span string and\n",
    "            don't care about official metrics, you can omit the ``id`` key.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        span_start_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span start position.\n",
    "        span_start_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_start_logits)``.\n",
    "        span_end_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, passage_length)`` representing unnormalised log\n",
    "            probabilities of the span end position (inclusive).\n",
    "        span_end_probs : torch.FloatTensor\n",
    "            The result of ``softmax(span_end_logits)``.\n",
    "        best_span : torch.IntTensor\n",
    "            The result of a constrained inference over ``span_start_logits`` and\n",
    "            ``span_end_logits`` to find the most probable span.  Shape is ``(batch_size, 2)``.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        best_span_str : List[str]\n",
    "            If sufficient metadata was provided for the instances in the batch, we also return the\n",
    "            string from the original passage that the model thinks is the best answer to the\n",
    "            question.\n",
    "        \"\"\"\n",
    "        embedded_question = self._highway_layer(self._text_field_embedder(question))\n",
    "        embedded_passage = self._highway_layer(self._text_field_embedder(passage))\n",
    "        batch_size = embedded_question.size(0)\n",
    "        passage_length = embedded_passage.size(1)\n",
    "        question_mask = util.get_text_field_mask(question).float()\n",
    "        passage_mask = util.get_text_field_mask(passage).float()\n",
    "        question_lstm_mask = question_mask if self._mask_lstms else None\n",
    "        passage_lstm_mask = passage_mask if self._mask_lstms else None\n",
    "\n",
    "        # We use question_last_state to initialize state controller\n",
    "        question_encoding, question_last_state = self._phrase_layer(embedded_question, question_lstm_mask)\n",
    "        passage_encoding, _ =  self._phrase_layer(embedded_passage, passage_lstm_mask)\n",
    "\n",
    "        encoded_question = self._dropout(question_encoding)\n",
    "        encoded_passage = self._dropout(passage_encoding)\n",
    "        encoding_dim = encoded_question.size(-1)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n",
    "        # Shape: (batch_size, passage_length, question_length)\n",
    "        passage_question_attention = util.last_dim_softmax(passage_question_similarity, question_mask)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n",
    "\n",
    "        # We replace masked values with something really negative here, so they don't affect the\n",
    "        # max below.\n",
    "        masked_similarity = util.replace_masked_values(passage_question_similarity,\n",
    "                                                       question_mask.unsqueeze(1),\n",
    "                                                       -1e7)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_similarity = masked_similarity.max(dim=-1)[0].squeeze(-1)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        question_passage_attention = util.masked_softmax(question_passage_similarity, passage_mask)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        question_passage_vector = util.weighted_sum(encoded_passage, question_passage_attention)\n",
    "        # Shape: (batch_size, passage_length, encoding_dim)\n",
    "        tiled_question_passage_vector = question_passage_vector.unsqueeze(1).expand(batch_size,\n",
    "                                                                                    passage_length,\n",
    "                                                                                    encoding_dim)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, encoding_dim * 4)\n",
    "        final_merged_passage = torch.cat([encoded_passage,\n",
    "                                          passage_question_vectors,\n",
    "                                          encoded_passage * passage_question_vectors,\n",
    "                                          encoded_passage * tiled_question_passage_vector],\n",
    "                                         dim=-1)\n",
    "\n",
    "        modeled_passage = self._dropout(self._modeling_layer(final_merged_passage, passage_lstm_mask))\n",
    "        modeling_dim = modeled_passage.size(-1)\n",
    "\n",
    "        # !!! modelled passage = M\n",
    "        M = modeled_passage\n",
    "\n",
    "        reasoner_last_state = self.reasoner.forward(question_last_state, M)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, modeling_dim)\n",
    "        tiled_reasoner_last_state = reasoner_last_state.unsqueeze(1).expand(batch_size,\n",
    "                                                                                   passage_length,\n",
    "                                                                                   modeling_dim)\n",
    "\n",
    "        # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))\n",
    "        answer_ready_representation = self._dropout(torch.cat([modeled_passage, modeled_passage*tiled_reasoner_last_state], dim=-1))\n",
    "\n",
    "        # ! Start prediction\n",
    "\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_start_logits = self._span_start_predictor(answer_ready_representation).squeeze(-1)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_start_probs = util.masked_softmax(span_start_logits, passage_mask)\n",
    "\n",
    "        # ! End prediction\n",
    "\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_end_logits = self._span_end_predictor(answer_ready_representation).squeeze(-1)\n",
    "        # Shape: (batch_size, passage_length)\n",
    "        span_end_probs = util.masked_softmax(span_start_logits, passage_mask)\n",
    "\n",
    "        span_start_logits = util.replace_masked_values(span_start_logits, passage_mask, -1e7)\n",
    "        span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)\n",
    "        best_span = self._get_best_span(span_start_logits, span_end_logits)\n",
    "\n",
    "        output_dict = {\"span_start_logits\": span_start_logits,\n",
    "                       \"span_start_probs\": span_start_probs,\n",
    "                       \"span_end_logits\": span_end_logits,\n",
    "                       \"span_end_probs\": span_end_probs,\n",
    "                       \"best_span\": best_span}\n",
    "        if span_start is not None:\n",
    "            loss = nll_loss(util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))\n",
    "            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))\n",
    "            loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))\n",
    "            self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))\n",
    "            self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "        if metadata is not None:\n",
    "            output_dict['best_span_str'] = []\n",
    "            for i in range(batch_size):\n",
    "                passage_str = metadata[i]['original_passage']\n",
    "                offsets = metadata[i]['token_offsets']\n",
    "                predicted_span = tuple(best_span[i].data.cpu().numpy())\n",
    "                start_offset = offsets[predicted_span[0]][0]\n",
    "                end_offset = offsets[predicted_span[1]][1]\n",
    "                best_span_string = passage_str[start_offset:end_offset]\n",
    "                output_dict['best_span_str'].append(best_span_string)\n",
    "                answer_texts = metadata[i].get('answer_texts', [])\n",
    "                if answer_texts:\n",
    "                    self._squad_metrics(best_span_string, answer_texts)\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        exact_match, f1_score = self._squad_metrics.get_metric(reset)\n",
    "        return {\n",
    "                'start_acc': self._span_start_accuracy.get_metric(reset),\n",
    "                'end_acc': self._span_end_accuracy.get_metric(reset),\n",
    "                'span_acc': self._span_accuracy.get_metric(reset),\n",
    "                'em': exact_match,\n",
    "                'f1': f1_score,\n",
    "                }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_span(span_start_logits: Variable, span_end_logits: Variable) -> Variable:\n",
    "        if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n",
    "            raise ValueError(\"Input shapes must be (batch_size, passage_length)\")\n",
    "        batch_size, passage_length = span_start_logits.size()\n",
    "        max_span_log_prob = [-1e20] * batch_size\n",
    "        span_start_argmax = [0] * batch_size\n",
    "        best_word_span = Variable(span_start_logits.data.new()\n",
    "                                  .resize_(batch_size, 2).fill_(0)).long()\n",
    "\n",
    "        span_start_logits = span_start_logits.data.cpu().numpy()\n",
    "        span_end_logits = span_end_logits.data.cpu().numpy()\n",
    "\n",
    "        for b in range(batch_size):  # pylint: disable=invalid-name\n",
    "            for j in range(passage_length):\n",
    "                val1 = span_start_logits[b, span_start_argmax[b]]\n",
    "                if val1 < span_start_logits[b, j]:\n",
    "                    span_start_argmax[b] = j\n",
    "                    val1 = span_start_logits[b, j]\n",
    "\n",
    "                val2 = span_end_logits[b, j]\n",
    "\n",
    "                if val1 + val2 > max_span_log_prob[b]:\n",
    "                    best_word_span[b, 0] = span_start_argmax[b]\n",
    "                    best_word_span[b, 1] = j\n",
    "                    max_span_log_prob[b] = val1 + val2\n",
    "        return best_word_span\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BidirectionalAttentionFlow':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
    "        num_highway_layers = params.pop(\"num_highway_layers\")\n",
    "        state_controller = Seq2SeqEncoder.from_params(params.pop(\"state_controller\"))\n",
    "        phrase_layer = Seq2SeqEncoder.from_params(params.pop(\"phrase_layer\"))\n",
    "        similarity_function = SimilarityFunction.from_params(params.pop(\"similarity_function\"))\n",
    "        modeling_layer = Seq2SeqEncoder.from_params(params.pop(\"modeling_layer\"))\n",
    "        dropout = params.pop('dropout', 0.2)\n",
    "\n",
    "        # TODO: Remove the following when fully deprecated\n",
    "        evaluation_json_file = params.pop('evaluation_json_file', None)\n",
    "        if evaluation_json_file is not None:\n",
    "            logger.warning(\"the 'evaluation_json_file' model parameter is deprecated, please remove\")\n",
    "\n",
    "        init_params = params.pop('initializer', None)\n",
    "        reg_params = params.pop('regularizer', None)\n",
    "        initializer = (InitializerApplicator.from_params(init_params)\n",
    "                       if init_params is not None\n",
    "                       else InitializerApplicator())\n",
    "        regularizer = RegularizerApplicator.from_params(reg_params) if reg_params is not None else None\n",
    "\n",
    "        mask_lstms = params.pop('mask_lstms', True)\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   num_highway_layers=num_highway_layers,\n",
    "                   state_controller=state_controller,\n",
    "                   phrase_layer=phrase_layer,\n",
    "                   attention_similarity_function=similarity_function,\n",
    "                   modeling_layer=modeling_layer,\n",
    "                   dropout=dropout,\n",
    "                   mask_lstms=mask_lstms,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import percache\n",
    "\n",
    "from allennlp.common.checks import ensure_pythonhashseed_set\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.tee_logger import TeeLogger\n",
    "from allennlp.common.util import prepare_environment\n",
    "from allennlp.data import Dataset, Vocabulary\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.archival import archive_model\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from allennlp.commands.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialization_dir = './serialization_dir'\n",
    "cache_dir = './cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "random_seed = 13370\n",
      "numpy_seed = 1337\n",
      "pytorch_seed = 133\n",
      "100%|##########| 442/442 [00:49<00:00,  8.90it/s]\n",
      "100%|##########| 48/48 [00:06<00:00,  6.98it/s]\n",
      "100%|##########| 98169/98169 [03:27<00:00, 473.92it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_environment(params)\n",
    "\n",
    "os.makedirs(serialization_dir, exist_ok=True)\n",
    "sys.stdout = TeeLogger(os.path.join(serialization_dir, \"stdout.log\"), sys.stdout)  # type: ignore\n",
    "sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), sys.stderr)  # type: ignore\n",
    "handler = logging.FileHandler(os.path.join(serialization_dir, \"python_logging.log\"))\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n",
    "logging.getLogger().addHandler(handler)\n",
    "serialization_params = deepcopy(params).as_dict(quiet=True)\n",
    "with open(os.path.join(serialization_dir, \"model_params.json\"), \"w\") as param_file:\n",
    "    json.dump(serialization_params, param_file, indent=4)\n",
    "\n",
    "cache = percache.Cache(cache_dir)\n",
    "\n",
    "# Now we begin assembling the required parts for the Trainer.\n",
    "dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n",
    "train_data_path = params.pop('train_data_path')\n",
    "logger.info(\"Reading training data from %s\", train_data_path)\n",
    "train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "validation_data_path = params.pop('validation_data_path', None)\n",
    "if validation_data_path is not None:\n",
    "    logger.info(\"Reading validation data from %s\", validation_data_path)\n",
    "    validation_data = dataset_reader.read(validation_data_path)\n",
    "    combined_data = Dataset(train_data.instances + validation_data.instances)\n",
    "else:\n",
    "    validation_data = None\n",
    "    combined_data = train_data\n",
    "\n",
    "vocab = cache(Vocabulary.from_params)(params.pop(\"vocabulary\", {}), combined_data)\n",
    "iterator = cache(DataIterator.from_params)(params.pop(\"iterator\"))\n",
    "\n",
    "cache.close()\n",
    "\n",
    "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e169975c9c98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                               trainer_params)\n\u001b[1;32m     14\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base train command'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Now tar up results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/allen institute/allennlp/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/allen institute/allennlp/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/allen institute/allennlp/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_batch_loss\u001b[0;34m(self, batch, for_training)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0malso\u001b[0m \u001b[0mapplies\u001b[0m \u001b[0mregularization\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/allen institute/allennlp/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, batch, for_training)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mtensor_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays_to_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtensor_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_description_from_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-585de93f1b23>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, question, passage, span_start, span_end, metadata)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0manswer_ready_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodeled_passage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodeled_passage\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtiled_reasoner_last_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# ! Start prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/autograd/_functions/basic_ops.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, a, b)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model.from_params(vocab, params.pop('model'))\n",
    "\n",
    "train_data.index_instances(vocab)\n",
    "if validation_data:\n",
    "    validation_data.index_instances(vocab)\n",
    "\n",
    "trainer_params = params.pop(\"trainer\")\n",
    "trainer = Trainer.from_params(model,\n",
    "                              serialization_dir,\n",
    "                              iterator,\n",
    "                              train_data,\n",
    "                              validation_data,\n",
    "                              trainer_params)\n",
    "params.assert_empty('base train command')\n",
    "trainer.train()\n",
    "\n",
    "# Now tar up results\n",
    "archive_model(serialization_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
