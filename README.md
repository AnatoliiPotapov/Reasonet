Reasonet implementation in pytorch
----------------------------------
(not yet ready)

The preprocessing code is originally from https://github.com/facebookresearch/DrQA
Most of the pytorch model code is borrowed from [Facebook/ParlAI](https://github.com/facebookresearch/ParlAI/) under a BSD-3 license.

Changes:

- Peprocessing and training without pos, ner and any additional features is now supported


## Requirements
- python >=3.5 
- pytorch 0.2.0 (please refer to [the previous version](https://github.com/hitvoice/DrQA/tree/bc0152c7ad69c56fda23f50adabd4355559b3a74) if you use pytorch 0.1.12)
- numpy
- pandas
- msgpack
- spacy 1.x

## Quick Start
### Setup
- download the project via `git clone https://github.com/AnatoliiPotapov/Reasonet.git; cd DrQA`
- make sure python 3 and pip is installed.
- install [pytorch](http://pytorch.org/) matched with your OS, python and cuda versions.
- install the remaining requirements via `pip install -r requirements.txt`
- download the SQuAD datafile, GloVe word vectors and Spacy English language models using `bash download.sh`.

### Train

```bash
# prepare the data
python prepro.py --pos False --ner False --tf False --wq False
# train for 20 epoches with batchsize 32
python train.py -e 20 -bs 32 --pos False --ner False --features False
```

### Detailed Comparisons (DRQA model)

Compared to what's described in the original paper:
- The grammatical features are generated by SpaCy instead of Stanford Core NLP. It's much faster (5 minutes vs 20 hours) but less accurate.
- The training samples are shuffled completely in each epoch. Performance degrades significantly when sorting the samples by length, dividing into mini-batches and then shuffle the mini-batches as recorded in the paper.
- The original paper does not make it clear whether POS and NER is a one-hot feature or has its own trainable embedding matrix. This implementation treats these two tags as discrete features with their own embedding matrices, which is found to be better in performance and makes the model more flexible.

Compared to the code in ParlAI:
- The DrQA model is not longer wrapped in a chatbot framework, which makes the code more readable, easier to modify and is faster to train. The preprocessing for text corpus is performed only once, while in a dialog framework raw text is transmitted each time and preprocessing for the same text must be done again and again.
- This is a full implementation of the original paper, while the model in ParlAI is a partial implementation, missing all grammatical features (lemma, POS tags and named entity tags). 
- When tuning top-k embeddings, the model will tune the embeddings of top-k question words as the original paper states, while the word dictionary in ParlAI is sorted by the frequency of all words. This does make a difference (see the discussion above).
- Some minor bug fixes and enhancements. Some of them have been merged into ParlAI.

Compared to the code in facebookresearch/DrQA:
- This project is much more light-weighted, while lacking the document retriever, the inference and interactive inference API, the extendibility to other datasets and some other enhancements.
- The implementation in facebookresearch/DrQA tokenizes the dataset using a Java-coded Stanford CoreNLP, while in this project we use a faster and simpler Spacy.
- The implementation in facebookresearch/DrQA treats the POS and NER tags as one-hot features, this implementation treats these two tags as discrete features with their own embedding matrice.
- The implementation in facebookresearch/DrQA is able to train on multiple GPUs, while (currently and for simplicity) in this implementation we only support single-GPU training.

